#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´¢ç»æ–°é—»çˆ¬è™«æ¨¡å—

åŠŸèƒ½ï¼š
1. ä»åŒèŠ±é¡ºè´¢ç»æ–°é—»æŠ“å–æ¯æ—¥æ–°é—»
2. ä»ä¸œæ–¹è´¢å¯Œè´¢ç»æ–°é—»æŠ“å–æ¯æ—¥æ–°é—»
3. æ–°é—»æ•°æ®æ¸…æ´—å’Œå¤„ç†
4. æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘æŠ“å–

æ”¯æŒç½‘ç«™ï¼š
- åŒèŠ±é¡ºè´¢ç»ï¼šhttps://news.10jqka.com.cn/
- ä¸œæ–¹è´¢å¯Œï¼šhttps://finance.eastmoney.com/news/
"""

import requests
import logging
from bs4 import BeautifulSoup
from typing import List, Dict
from datetime import datetime
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FinanceNewsCrawler:
    """è´¢ç»æ–°é—»çˆ¬è™«ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.session = requests.Session()
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def crawl_tonghuashun_news(self, limit: int = 20) -> List[Dict]:
        """
        çˆ¬å–åŒèŠ±é¡ºè´¢ç»æ–°é—»
        
        Args:
            limit: è·å–æ–°é—»æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: æ–°é—»åˆ—è¡¨ï¼Œæ¯ä¸ªæ–°é—»åŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€æ—¶é—´ç­‰ä¿¡æ¯
        """
        news_list = []
        
        try:
            logger.info("ğŸ” å¼€å§‹çˆ¬å–åŒèŠ±é¡ºè´¢ç»æ–°é—»...")
            
            # åŒèŠ±é¡ºè´¢ç»æ–°é—»é¡µé¢
            urls = [
                'https://news.10jqka.com.cn/cjzx_list/',  # è´¢ç»èµ„è®¯
                'https://news.10jqka.com.cn/stock_list/',  # ä¸ªè‚¡æ–°é—»
                'https://news.10jqka.com.cn/market_list/'  # å¸‚åœºæ–°é—»
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    response.encoding = 'utf-8'
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # è§£ææ–°é—»åˆ—è¡¨
                    news_items = soup.find_all('div', class_='list-item') or soup.find_all('li', class_='item')
                    
                    for item in news_items[:limit//len(urls)]:
                        try:
                            # æå–æ ‡é¢˜å’Œé“¾æ¥
                            title_link = item.find('a')
                            if title_link:
                                title = title_link.get_text(strip=True)
                                link = title_link.get('href', '')
                                
                                # è¡¥å…¨é“¾æ¥
                                if link.startswith('/'):
                                    link = 'https://news.10jqka.com.cn' + link
                                elif not link.startswith('http'):
                                    continue
                                
                                # æå–æ—¶é—´
                                time_elem = item.find('span', class_='time') or item.find('em')
                                pub_time = time_elem.get_text(strip=True) if time_elem else datetime.now().strftime('%Y-%m-%d %H:%M')
                                
                                news_item = {
                                    'title': title,
                                    'link': link,
                                    'source': 'åŒèŠ±é¡º',
                                    'pub_time': pub_time,
                                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                }
                                
                                news_list.append(news_item)
                                
                        except Exception as e:
                            logger.debug(f"è§£æå•æ¡åŒèŠ±é¡ºæ–°é—»å¤±è´¥: {e}")
                            continue
                            
                    time.sleep(1)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    logger.warning(f"çˆ¬å–åŒèŠ±é¡ºURL {url} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸçˆ¬å–åŒèŠ±é¡ºæ–°é—» {len(news_list)} æ¡")
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–åŒèŠ±é¡ºæ–°é—»å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_eastmoney_news(self, limit: int = 20) -> List[Dict]:
        """
        çˆ¬å–ä¸œæ–¹è´¢å¯Œè´¢ç»æ–°é—»
        
        Args:
            limit: è·å–æ–°é—»æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: æ–°é—»åˆ—è¡¨
        """
        news_list = []
        
        try:
            logger.info("ğŸ” å¼€å§‹çˆ¬å–ä¸œæ–¹è´¢å¯Œè´¢ç»æ–°é—»...")
            
            # ä¸œæ–¹è´¢å¯Œæ–°é—»é¡µé¢
            urls = [
                'https://finance.eastmoney.com/news/cjxw.html',  # è´¢ç»æ–°é—»
                'https://finance.eastmoney.com/news/cgsxw.html',  # ä¸ªè‚¡æ–°é—»
                'https://finance.eastmoney.com/news/cschxw.html'  # å¸‚åœºæ–°é—»
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    response.encoding = 'utf-8'
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # è§£ææ–°é—»åˆ—è¡¨
                    news_items = soup.find_all('div', class_='text-line') or soup.find_all('li')
                    
                    for item in news_items[:limit//len(urls)]:
                        try:
                            # æå–æ ‡é¢˜å’Œé“¾æ¥
                            title_link = item.find('a')
                            if title_link:
                                title = title_link.get_text(strip=True)
                                link = title_link.get('href', '')
                                
                                # è¡¥å…¨é“¾æ¥
                                if link.startswith('/'):
                                    link = 'https://finance.eastmoney.com' + link
                                elif not link.startswith('http'):
                                    continue
                                
                                # æå–æ—¶é—´
                                time_elem = item.find('span', class_='time') or item.find('span', class_='date')
                                pub_time = time_elem.get_text(strip=True) if time_elem else datetime.now().strftime('%Y-%m-%d %H:%M')
                                
                                news_item = {
                                    'title': title,
                                    'link': link,
                                    'source': 'ä¸œæ–¹è´¢å¯Œ',
                                    'pub_time': pub_time,
                                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                }
                                
                                news_list.append(news_item)
                                
                        except Exception as e:
                            logger.debug(f"è§£æå•æ¡ä¸œæ–¹è´¢å¯Œæ–°é—»å¤±è´¥: {e}")
                            continue
                            
                    time.sleep(1)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    logger.warning(f"çˆ¬å–ä¸œæ–¹è´¢å¯ŒURL {url} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸçˆ¬å–ä¸œæ–¹è´¢å¯Œæ–°é—» {len(news_list)} æ¡")
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ä¸œæ–¹è´¢å¯Œæ–°é—»å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_sina_finance_news(self, limit: int = 15) -> List[Dict]:
        """
        çˆ¬å–æ–°æµªè´¢ç»æ–°é—»ï¼ˆå¤‡ç”¨æ•°æ®æºï¼‰
        
        Args:
            limit: è·å–æ–°é—»æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: æ–°é—»åˆ—è¡¨
        """
        news_list = []
        
        try:
            logger.info("ğŸ” å¼€å§‹çˆ¬å–æ–°æµªè´¢ç»æ–°é—»...")
            
            url = 'https://finance.sina.com.cn/'
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»é“¾æ¥
            news_links = soup.find_all('a', href=True)
            
            count = 0
            for link in news_links:
                if count >= limit:
                    break
                    
                try:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # ç­›é€‰è´¢ç»ç›¸å…³æ–°é—»
                    if ('finance.sina.com.cn' in href or 'stock.finance.sina.com.cn' in href) and \
                       len(title) > 10 and any(keyword in title for keyword in ['è‚¡ç¥¨', 'è‚¡å¸‚', 'è´¢ç»', 'æŠ•èµ„', 'å¸‚åœº', 'ç»æµ']):
                        
                        news_item = {
                            'title': title,
                            'link': href,
                            'source': 'æ–°æµªè´¢ç»',
                            'pub_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        }
                        
                        news_list.append(news_item)
                        count += 1
                        
                except Exception as e:
                    logger.debug(f"è§£ææ–°æµªè´¢ç»æ–°é—»å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸçˆ¬å–æ–°æµªè´¢ç»æ–°é—» {len(news_list)} æ¡")
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–æ–°æµªè´¢ç»æ–°é—»å¤±è´¥: {e}")
        
        return news_list
    
    def get_all_finance_news(self, limit_per_source: int = 20) -> List[Dict]:
        """
        è·å–æ‰€æœ‰è´¢ç»æ–°é—»ï¼ˆå¤šçº¿ç¨‹å¹¶å‘çˆ¬å–ï¼‰
        
        Args:
            limit_per_source: æ¯ä¸ªæ•°æ®æºçš„æ–°é—»æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: åˆå¹¶åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å–æ‰€æœ‰è´¢ç»æ–°é—»...")
        
        all_news = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘çˆ¬å–
        with ThreadPoolExecutor(max_workers=3) as executor:
            # æäº¤çˆ¬å–ä»»åŠ¡
            futures = {
                executor.submit(self.crawl_tonghuashun_news, limit_per_source): 'åŒèŠ±é¡º',
                executor.submit(self.crawl_eastmoney_news, limit_per_source): 'ä¸œæ–¹è´¢å¯Œ',
                executor.submit(self.crawl_sina_finance_news, limit_per_source): 'æ–°æµªè´¢ç»'
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                source_name = futures[future]
                try:
                    news_data = future.result()
                    if news_data:
                        all_news.extend(news_data)
                        logger.info(f"âœ… {source_name} çˆ¬å–å®Œæˆï¼Œè·å¾— {len(news_data)} æ¡æ–°é—»")
                    else:
                        logger.warning(f"âš ï¸ {source_name} æœªè·å–åˆ°æ–°é—»æ•°æ®")
                        
                except Exception as e:
                    logger.error(f"âŒ {source_name} çˆ¬å–å¤±è´¥: {e}")
        
        # å»é‡å¤„ç†ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
        seen_titles = set()
        unique_news = []
        
        for news in all_news:
            title = news.get('title', '')
            if title not in seen_titles and len(title) > 5:
                seen_titles.add(title)
                unique_news.append(news)
        
        logger.info(f"ğŸ‰ è´¢ç»æ–°é—»çˆ¬å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š åŸå§‹æ–°é—»: {len(all_news)} æ¡")
        logger.info(f"   ğŸ“Š å»é‡åæ–°é—»: {len(unique_news)} æ¡")
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_counts = {}
        for news in unique_news:
            source = news.get('source', 'æœªçŸ¥')
            source_counts[source] = source_counts.get(source, 0) + 1
        
        logger.info("ğŸ“ˆ æ–°é—»æ¥æºåˆ†å¸ƒï¼š")
        for source, count in source_counts.items():
            logger.info(f"   {source}: {count} æ¡")
        
        return unique_news
    
    def filter_stock_related_news(self, news_list: List[Dict], 
                                 stock_keywords: List[str] = None) -> List[Dict]:
        """
        ç­›é€‰è‚¡ç¥¨ç›¸å…³æ–°é—»
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            stock_keywords: è‚¡ç¥¨ç›¸å…³å…³é”®è¯åˆ—è¡¨
            
        Returns:
            List[Dict]: ç­›é€‰åçš„æ–°é—»åˆ—è¡¨
        """
        if stock_keywords is None:
            stock_keywords = [
                'è‚¡ç¥¨', 'è‚¡å¸‚', 'Aè‚¡', 'æ¸¯è‚¡', 'æ²ªæ·±', 'ä¸Šè¯', 'æ·±è¯', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›æ¿',
                'æ¶¨åœ', 'è·Œåœ', 'ç‰›å¸‚', 'ç†Šå¸‚', 'IPO', 'é‡ç»„', 'å¹¶è´­', 'åˆ†çº¢', 'é…è‚¡',
                'æœºæ„', 'åŸºé‡‘', 'åˆ¸å•†', 'é“¶è¡Œ', 'ä¿é™©', 'åœ°äº§', 'ç§‘æŠ€', 'åŒ»è¯', 'æ–°èƒ½æº',
                'èŠ¯ç‰‡', 'äººå·¥æ™ºèƒ½', '5G', 'æ–°èƒ½æºæ±½è½¦', 'é”‚ç”µæ± ', 'å…‰ä¼', 'é£ç”µ'
            ]
        
        filtered_news = []
        
        for news in news_list:
            title = news.get('title', '')
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«è‚¡ç¥¨ç›¸å…³å…³é”®è¯
            if any(keyword in title for keyword in stock_keywords):
                filtered_news.append(news)
        
        logger.info(f"ğŸ“Š è‚¡ç¥¨ç›¸å…³æ–°é—»ç­›é€‰å®Œæˆï¼š{len(filtered_news)}/{len(news_list)} æ¡")
        
        return filtered_news
    
    def get_news_content(self, news_item: Dict) -> str:
        """
        è·å–æ–°é—»è¯¦ç»†å†…å®¹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        
        Args:
            news_item: æ–°é—»é¡¹ç›®
            
        Returns:
            str: æ–°é—»å†…å®¹
        """
        try:
            url = news_item.get('link', '')
            if not url:
                return ""
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•å¤šç§å¯èƒ½çš„å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '.article-content',
                '.content',
                '.news-content', 
                '#artibody',
                '.article-body',
                'div[class*="content"]'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    break
            
            # å¦‚æœæ‰¾ä¸åˆ°å†…å®¹ï¼Œä½¿ç”¨æ ‡é¢˜
            if not content:
                content = news_item.get('title', '')
            
            return content[:500]  # é™åˆ¶å†…å®¹é•¿åº¦
            
        except Exception as e:
            logger.debug(f"è·å–æ–°é—»å†…å®¹å¤±è´¥: {e}")
            return news_item.get('title', '')
    
    def format_news_for_summary(self, news_list: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æ–°é—»ç”¨äºå¤§æ¨¡å‹æ€»ç»“
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–åçš„æ–°é—»æ–‡æœ¬
        """
        if not news_list:
            return "ä»Šæ—¥æš‚æ— è´¢ç»è‚¡ç¥¨ç›¸å…³æ–°é—»ã€‚"
        
        formatted_text = f"ä»Šæ—¥è´¢ç»è‚¡ç¥¨æ–°é—»æ±‡æ€»ï¼ˆå…±{len(news_list)}æ¡ï¼‰ï¼š\n\n"
        
        for i, news in enumerate(news_list, 1):
            title = news.get('title', '')
            source = news.get('source', '')
            pub_time = news.get('pub_time', '')
            
            formatted_text += f"{i}. ã€{source}ã€‘{title}\n"
            if pub_time:
                formatted_text += f"   æ—¶é—´ï¼š{pub_time}\n"
            formatted_text += "\n"
        
        return formatted_text
    
    def get_daily_finance_news(self, limit_per_source: int = 15, 
                              filter_stock: bool = True) -> Dict:
        """
        è·å–æ¯æ—¥è´¢ç»æ–°é—»æ±‡æ€»
        
        Args:
            limit_per_source: æ¯ä¸ªæ•°æ®æºçš„æ–°é—»æ•°é‡
            filter_stock: æ˜¯å¦åªç­›é€‰è‚¡ç¥¨ç›¸å…³æ–°é—»
            
        Returns:
            Dict: åŒ…å«æ–°é—»åˆ—è¡¨å’Œæ±‡æ€»ä¿¡æ¯çš„å­—å…¸
        """
        start_time = datetime.now()
        
        try:
            # è·å–æ‰€æœ‰æ–°é—»
            all_news = self.get_all_finance_news(limit_per_source)
            
            # ç­›é€‰è‚¡ç¥¨ç›¸å…³æ–°é—»
            if filter_stock:
                filtered_news = self.filter_stock_related_news(all_news)
            else:
                filtered_news = all_news
            
            # æ ¼å¼åŒ–æ–°é—»
            formatted_text = self.format_news_for_summary(filtered_news)
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            result = {
                'news_list': filtered_news,
                'formatted_text': formatted_text,
                'total_count': len(filtered_news),
                'source_distribution': {},
                'crawl_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),
                'duration': str(duration)
            }
            
            # ç»Ÿè®¡å„æ¥æºæ•°é‡
            for news in filtered_news:
                source = news.get('source', 'æœªçŸ¥')
                result['source_distribution'][source] = result['source_distribution'].get(source, 0) + 1
            
            logger.info("ğŸ‰ æ¯æ—¥è´¢ç»æ–°é—»è·å–å®Œæˆï¼")
            logger.info(f"   ğŸ“Š æ–°é—»æ€»æ•°: {result['total_count']} æ¡")
            logger.info(f"   â±ï¸ çˆ¬å–è€—æ—¶: {duration}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ¯æ—¥è´¢ç»æ–°é—»å¤±è´¥: {e}")
            return {
                'news_list': [],
                'formatted_text': "ä»Šæ—¥è´¢ç»æ–°é—»è·å–å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                'total_count': 0,
                'source_distribution': {},
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'duration': '0',
                'error': str(e)
            }
