# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®è·å–å™¨æ¨¡å—
è´Ÿè´£ä»Tushare APIè·å–è‚¡ç¥¨æ•°æ®
"""

import tushare as ts
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import List, Optional

from config import TUSHARE_TOKEN

logger = logging.getLogger(__name__)


class StockDataFetcher:
    """è‚¡ç¥¨æ•°æ®è·å–å™¨"""
    
    def __init__(self, token: str = None):
        """
        åˆå§‹åŒ–æ•°æ®è·å–å™¨
        
        Args:
            token: Tushare API token
        """
        self.token = token or TUSHARE_TOKEN
        if not self.token or self.token == "your_tushare_token_here":
            raise ValueError("è¯·åœ¨config.pyä¸­è®¾ç½®æœ‰æ•ˆçš„Tushare token")
        
        # è®¾ç½®tushare token
        ts.set_token(self.token)
        self.pro = ts.pro_api()
        
    def get_daily_data(self, ts_code: str, start_date: str = None, 
                      end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001.SZï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            
        Returns:
            pd.DataFrame: æ—¥çº¿æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å– {ts_code} çš„æ—¥çº¿æ•°æ®...")
            
            # è·å–æ—¥çº¿æ•°æ®
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )
            
            if df.empty:
                logger.warning(f"è‚¡ç¥¨ {ts_code} åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            
            logger.info(f"æˆåŠŸè·å– {ts_code} çš„ {len(df)} æ¡æ—¥çº¿æ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å– {ts_code} æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_multiple_stocks_data(self, stock_codes: List[str], 
                                start_date: str = None, end_date: str = None,
                                batch_size: int = 50, delay: float = 0.5) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé˜²æ­¢APIè°ƒç”¨è¿‡äºé¢‘ç¹
            delay: æ¯æ¬¡è°ƒç”¨çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„æ—¥çº¿æ•°æ®
        """
        all_data = []
        total_stocks = len(stock_codes)
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {total_stocks} åªè‚¡ç¥¨æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        for i, ts_code in enumerate(stock_codes, 1):
            try:
                # è·å–å•åªè‚¡ç¥¨æ•°æ®
                df = self.get_daily_data(ts_code, start_date, end_date)
                if df is not None and not df.empty:
                    all_data.append(df)
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_stocks:
                    success_count = len(all_data)
                    logger.info(f"è¿›åº¦: {i}/{total_stocks} ({i/total_stocks*100:.1f}%), æˆåŠŸè·å–: {success_count}åª")
                
                # é¿å…é¢‘ç¹è°ƒç”¨APIï¼Œé€‚å½“ä¼‘çœ 
                import time
                time.sleep(delay)
                
                # æ¯æ‰¹æ¬¡åç¨é•¿ä¼‘çœ 
                if i % batch_size == 0:
                    logger.info(f"å®Œæˆç¬¬ {i//batch_size} æ‰¹æ¬¡ï¼Œä¼‘çœ 2ç§’...")
                    time.sleep(2.0)
                
            except Exception as e:
                logger.error(f"è·å–è‚¡ç¥¨ {ts_code} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.warning("æ²¡æœ‰è·å–åˆ°ä»»ä½•è‚¡ç¥¨æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        success_rate = len(all_data) / total_stocks * 100
        logger.info(f"æ‰¹é‡è·å–å®Œæˆï¼æ€»å…±è·å–äº† {len(combined_df)} æ¡è‚¡ç¥¨æ•°æ®è®°å½•")
        logger.info(f"æˆåŠŸç‡: {len(all_data)}/{total_stocks} ({success_rate:.1f}%)")
        
        return combined_df
    
    def get_daily_by_date(self, trade_date: str, ts_code: str = None) -> Optional[pd.DataFrame]:
        """
        æ ¹æ®äº¤æ˜“æ—¥æœŸè·å–å½“æ—¥æ‰€æœ‰è‚¡ç¥¨æˆ–æŒ‡å®šè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®
        
        è¿™ç§æ–¹å¼é€‚åˆï¼š
        - è·å–æŸä¸ªäº¤æ˜“æ—¥çš„å…¨å¸‚åœºæ•°æ®
        - å¯¹æ¯”æŸä¸ªäº¤æ˜“æ—¥ä¸åŒè‚¡ç¥¨çš„è¡¨ç°
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            ts_code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœæŒ‡å®šåˆ™åªè·å–è¯¥è‚¡ç¥¨æ•°æ®
            
        Returns:
            pd.DataFrame: å½“æ—¥è¡Œæƒ…æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å– {trade_date} çš„è¡Œæƒ…æ•°æ®...")
            
            # ä½¿ç”¨trade_dateå‚æ•°è·å–æ•°æ®
            df = self.pro.daily(
                trade_date=trade_date,
                ts_code=ts_code  # å¯ä»¥ä¸ºNoneè·å–å…¨å¸‚åœºæ•°æ®
            )
            
            if df.empty:
                logger.warning(f"äº¤æ˜“æ—¥ {trade_date} æ²¡æœ‰è¡Œæƒ…æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            
            if ts_code:
                logger.info(f"æˆåŠŸè·å– {ts_code} åœ¨ {trade_date} çš„æ•°æ®")
            else:
                logger.info(f"æˆåŠŸè·å– {trade_date} å…¨å¸‚åœº {len(df)} æ¡æ•°æ®")
            
            return df
            
        except Exception as e:
            logger.error(f"è·å– {trade_date} è¡Œæƒ…æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_latest_trading_day_data(self, stock_codes: List[str] = None) -> Optional[pd.DataFrame]:
        """
        è·å–æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–å…¨å¸‚åœºæ•°æ®
            
        Returns:
            pd.DataFrame: æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        """
        # è·å–æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥è¿›è¡Œå°è¯•
        today = datetime.now()
        for i in range(10):  # å°è¯•æœ€è¿‘10å¤©
            check_date = (today - timedelta(days=i)).strftime('%Y%m%d')
            
            try:
                if stock_codes:
                    # è·å–æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
                    all_data = []
                    for ts_code in stock_codes:
                        df = self.get_daily_by_date(check_date, ts_code)
                        if df is not None and not df.empty:
                            all_data.append(df)
                    
                    if all_data:
                        return pd.concat(all_data, ignore_index=True)
                else:
                    # è·å–å…¨å¸‚åœºæ•°æ®
                    return self.get_daily_by_date(check_date)
                    
            except:
                continue
        
        logger.warning("æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®")
        return None

    def get_stock_basic(self, exchange: str = None, is_hs: str = None, 
                       list_status: str = 'L', market: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        
        Args:
            exchange: äº¤æ˜“æ‰€ï¼ˆSSEä¸Šäº¤æ‰€ SZSEæ·±äº¤æ‰€ï¼‰
            is_hs: æ˜¯å¦æ²ªæ·±æ¸¯é€šæ ‡çš„ï¼ˆNå¦ Hæ²ªè‚¡é€š Sæ·±è‚¡é€šï¼‰
            list_status: ä¸Šå¸‚çŠ¶æ€ Lä¸Šå¸‚ Dé€€å¸‚ Pæš‚åœä¸Šå¸‚ï¼ˆé»˜è®¤Lï¼‰
            market: å¸‚åœºç±»å‹ ä¸»æ¿Main åˆ›ä¸šæ¿ChiNext ç§‘åˆ›æ¿STARï¼ˆé»˜è®¤Noneè·å–æ‰€æœ‰ï¼‰
            
        Returns:
            pd.DataFrame: è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        """
        try:
            logger.info("æ­£åœ¨è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
            df = self.pro.stock_basic(
                exchange=exchange, 
                is_hs=is_hs,
                list_status=list_status,
                market=market,
                fields='ts_code,symbol,name,area,industry,market,list_date,list_status'
            )
            logger.info(f"è·å–åˆ° {len(df)} åªè‚¡ç¥¨çš„åŸºç¡€ä¿¡æ¯")
            return df
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def get_main_board_stocks(self, use_cache: bool = True) -> List[str]:
        """
        è·å–Aè‚¡ä¸»æ¿è‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ–‡ä»¶
            
        Returns:
            List[str]: ä¸»æ¿è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        cache_file = 'main_board_stocks_cache.txt'
        
        # å°è¯•ä»ç¼“å­˜æ–‡ä»¶è¯»å–
        if use_cache:
            try:
                import os
                
                if os.path.exists(cache_file):
                    # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(cache_file))
                    if datetime.now() - file_mtime < timedelta(days=7):  # 7å¤©å†…çš„ç¼“å­˜æœ‰æ•ˆ
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            cached_stocks = [line.strip() for line in f.readlines() if line.strip()]
                        logger.info(f"ä»ç¼“å­˜æ–‡ä»¶è¯»å–åˆ° {len(cached_stocks)} åªä¸»æ¿è‚¡ç¥¨")
                        return cached_stocks
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        
        try:
            logger.info("æ­£åœ¨ä»APIè·å–Aè‚¡ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨...")
            
            # è·å–ä¸»æ¿è‚¡ç¥¨ï¼ˆæ’é™¤åˆ›ä¸šæ¿å’Œç§‘åˆ›æ¿ï¼‰
            df = self.pro.stock_basic(
                list_status='L',  # åªè¦ä¸Šå¸‚çš„è‚¡ç¥¨
                fields='ts_code,symbol,name,market,list_date'
            )
            
            if df is not None and not df.empty:
                # è¿‡æ»¤ä¸»æ¿è‚¡ç¥¨ï¼ˆæ’é™¤åˆ›ä¸šæ¿300å¼€å¤´ã€ç§‘åˆ›æ¿688å¼€å¤´ã€åŒ—äº¤æ‰€830/430å¼€å¤´ç­‰ï¼‰
                main_board_df = df[
                    (~df['ts_code'].str.startswith('300')) &  # æ’é™¤åˆ›ä¸šæ¿
                    (~df['ts_code'].str.startswith('688')) &  # æ’é™¤ç§‘åˆ›æ¿
                    (~df['ts_code'].str.startswith('830')) &  # æ’é™¤åŒ—äº¤æ‰€
                    (~df['ts_code'].str.startswith('430')) &  # æ’é™¤åŒ—äº¤æ‰€
                    (~df['ts_code'].str.startswith('200')) &  # æ’é™¤Bè‚¡
                    (~df['ts_code'].str.startswith('900'))    # æ’é™¤Bè‚¡
                ]
                
                stock_codes = main_board_df['ts_code'].tolist()
                logger.info(f"ä»APIè·å–åˆ° {len(stock_codes)} åªAè‚¡ä¸»æ¿è‚¡ç¥¨")
                
                # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        for code in stock_codes:
                            f.write(f"{code}\\n")
                    logger.info("è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶")
                except Exception as e:
                    logger.warning(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
                
                # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
                sh_count = len([code for code in stock_codes if code.endswith('.SH')])
                sz_count = len([code for code in stock_codes if code.endswith('.SZ')])
                logger.info(f"å…¶ä¸­ä¸Šäº¤æ‰€ä¸»æ¿: {sh_count}åª, æ·±äº¤æ‰€ä¸»æ¿: {sz_count}åª")
                
                return stock_codes
            else:
                logger.warning("æœªè·å–åˆ°è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
                return self._get_backup_main_board_stocks()
                
        except Exception as e:
            logger.error(f"è·å–ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            logger.info("ä½¿ç”¨å¤‡ç”¨ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨...")
            return self._get_backup_main_board_stocks()
    
    def _get_backup_main_board_stocks(self) -> List[str]:
        """
        å¤‡ç”¨çš„ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¸¸è§çš„å¤§ç›˜è‚¡ï¼‰
        å½“APIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨
        
        Returns:
            List[str]: å¤‡ç”¨è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        backup_stocks = [
            # æ²ªå¸‚ä¸»æ¿å¤§ç›˜è‚¡
            '600000.SH', '600036.SH', '600519.SH', '600887.SH', '601318.SH',
            '601398.SH', '601857.SH', '601988.SH', '600028.SH', '600030.SH',
            '600050.SH', '600104.SH', '600276.SH', '600690.SH', '600703.SH',
            '600837.SH', '600900.SH', '601012.SH', '601066.SH', '601166.SH',
            '601169.SH', '601229.SH', '601288.SH', '601328.SH', '601336.SH',
            '601390.SH', '601601.SH', '601628.SH', '601668.SH', '601688.SH',
            '601766.SH', '601788.SH', '601818.SH', '601828.SH', '601888.SH',
            '601898.SH', '601919.SH', '601939.SH', '601985.SH', '601989.SH',
            
            # æ·±å¸‚ä¸»æ¿å¤§ç›˜è‚¡
            '000001.SZ', '000002.SZ', '000063.SZ', '000100.SZ', '000157.SZ',
            '000166.SZ', '000333.SZ', '000338.SZ', '000858.SZ', '000895.SZ',
            '000938.SZ', '000961.SZ', '001979.SZ', '002001.SZ', '002007.SZ',
            '002024.SZ', '002027.SZ', '002032.SZ', '002142.SZ', '002202.SZ',
            '002230.SZ', '002236.SZ', '002241.SZ', '002304.SZ', '002352.SZ',
            '002415.SZ', '002456.SZ', '002475.SZ', '002493.SZ', '002508.SZ',
            '002594.SZ', '002601.SZ', '002602.SZ', '002714.SZ', '002736.SZ',
            '002791.SZ', '002812.SZ', '002841.SZ', '002867.SZ', '002916.SZ',
        ]
        
        logger.info(f"ä½¿ç”¨å¤‡ç”¨è‚¡ç¥¨åˆ—è¡¨ï¼ŒåŒ…å« {len(backup_stocks)} åªä¸»è¦çš„ä¸»æ¿è‚¡ç¥¨")
        return backup_stocks
    
    def get_trade_calendar(self, start_date: str, end_date: str, exchange: str = 'SSE') -> Optional[pd.DataFrame]:
        """
        è·å–äº¤æ˜“æ—¥å†
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            exchange: äº¤æ˜“æ‰€ï¼ˆSSEä¸Šäº¤æ‰€ SZSEæ·±äº¤æ‰€ï¼‰
            
        Returns:
            pd.DataFrame: äº¤æ˜“æ—¥å†æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å– {start_date} åˆ° {end_date} çš„äº¤æ˜“æ—¥å†...")
            
            df = self.pro.trade_cal(
                exchange=exchange,
                is_open='1',  # åªè·å–äº¤æ˜“æ—¥
                start_date=start_date,
                end_date=end_date,
                fields='cal_date'
            )
            
            if df is not None and not df.empty:
                logger.info(f"è·å–åˆ° {len(df)} ä¸ªäº¤æ˜“æ—¥")
                return df
            else:
                logger.warning("æœªè·å–åˆ°äº¤æ˜“æ—¥å†æ•°æ®")
                return None
                
        except Exception as e:
            logger.error(f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            return None
    
    def get_daily_with_retry(self, ts_code: str = '', trade_date: str = '', 
                           start_date: str = '', end_date: str = '', max_retries: int = 3) -> Optional[pd.DataFrame]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ—¥çº¿æ•°æ®è·å–
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            trade_date: äº¤æ˜“æ—¥æœŸ
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            pd.DataFrame: æ—¥çº¿æ•°æ®
        """
        import time
        
        for retry in range(max_retries):
            try:
                if trade_date:
                    df = self.pro.daily(ts_code=ts_code, trade_date=trade_date)
                else:
                    df = self.pro.daily(ts_code=ts_code, start_date=start_date, end_date=end_date)
                
                if df is not None:
                    # æ•°æ®é¢„å¤„ç†
                    if not df.empty:
                        df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
                    return df
                    
            except Exception as e:
                logger.warning(f"ç¬¬ {retry + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if retry < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•
                    time.sleep(1 + retry)  # é€’å¢å»¶è¿Ÿ
                else:
                    logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œè·å–å¤±è´¥")
        
        return None
    
    def get_all_market_data_by_dates(self, start_date: str, end_date: str, 
                                   delay: float = 0.5, exchange: str = 'SSE') -> pd.DataFrame:
        """
        é€šè¿‡äº¤æ˜“æ—¥å¾ªç¯è·å–å…¨å¸‚åœºå†å²æ•°æ®ï¼ˆæ¨èç”¨äºå¤§æ‰¹é‡æ•°æ®è·å–ï¼‰
        
        è¿™ç§æ–¹å¼çš„ä¼˜åŠ¿ï¼š
        - è‚¡ç¥¨æœ‰5000+åªï¼Œæ¯å¹´äº¤æ˜“æ—¥åªæœ‰220å·¦å³ï¼Œå¾ªç¯æ¬¡æ•°å°‘
        - æ¯æ¬¡è·å–ä¸€å¤©çš„å…¨å¸‚åœºæ•°æ®ï¼Œæ•ˆç‡æ›´é«˜
        - æ›´ç¨³å®šï¼Œä¸å®¹æ˜“è§¦å‘APIé™åˆ¶
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            delay: æ¯æ¬¡è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
            exchange: äº¤æ˜“æ‰€
            
        Returns:
            pd.DataFrame: å…¨å¸‚åœºå†å²æ•°æ®
        """
        import time
        
        # è·å–äº¤æ˜“æ—¥å†
        trade_cal = self.get_trade_calendar(start_date, end_date, exchange)
        if trade_cal is None or trade_cal.empty:
            logger.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œé€€å‡ºæ•°æ®è·å–")
            return pd.DataFrame()
        
        trading_days = trade_cal['cal_date'].values
        total_days = len(trading_days)
        logger.info(f"å¼€å§‹é€šè¿‡äº¤æ˜“æ—¥å¾ªç¯è·å–æ•°æ®ï¼Œå…± {total_days} ä¸ªäº¤æ˜“æ—¥")
        
        all_data = []
        successful_days = 0
        
        for i, trade_date in enumerate(trading_days, 1):
            try:
                logger.info(f"æ­£åœ¨è·å– {trade_date} çš„å…¨å¸‚åœºæ•°æ® ({i}/{total_days})")
                
                # ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–æ•°æ®
                df = self.get_daily_with_retry(trade_date=trade_date)
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    successful_days += 1
                    logger.info(f"æˆåŠŸè·å– {trade_date} çš„ {len(df)} åªè‚¡ç¥¨æ•°æ®")
                else:
                    logger.warning(f"æœªè·å–åˆ° {trade_date} çš„æ•°æ®")
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_days:
                    success_rate = successful_days / i * 100
                    logger.info(f"è¿›åº¦: {i}/{total_days} ({i/total_days*100:.1f}%), "
                              f"æˆåŠŸè·å–: {successful_days}å¤© ({success_rate:.1f}%)")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"è·å– {trade_date} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•äº¤æ˜“æ—¥æ•°æ®")
            return pd.DataFrame()

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        total_records = len(combined_df)
        unique_stocks = combined_df['ts_code'].nunique() if 'ts_code' in combined_df.columns else 0
        
        logger.info(f"ğŸ‰ å…¨å¸‚åœºæ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»è®°å½•æ•°: {total_records:,} æ¡")
        logger.info(f"   ğŸ“ˆ æ¶‰åŠè‚¡ç¥¨: {unique_stocks} åª") 
        logger.info(f"   ğŸ“… äº¤æ˜“æ—¥æ•°: {successful_days}/{total_days}")
        logger.info(f"   âœ… æˆåŠŸç‡: {successful_days/total_days*100:.1f}%")
        
        return combined_df
    
    def get_all_market_data_by_dates_with_batch_insert(self, start_date: str, end_date: str, 
                                                      delay: float = 0.5, exchange: str = 'SSE',
                                                      db_instance=None, batch_days: int = 10) -> dict:
        """
        é€šè¿‡äº¤æ˜“æ—¥å¾ªç¯è·å–å…¨å¸‚åœºå†å²æ•°æ®ï¼Œå¹¶åˆ†æ‰¹æ’å…¥æ•°æ®åº“ï¼ˆæ¨èç”¨äºå¤§æ‰¹é‡æ•°æ®ï¼‰
        
        ä¼˜åŠ¿ï¼š
        - æŒ‰äº¤æ˜“æ—¥æ‰¹é‡æ’å…¥ï¼Œé¿å…å†…å­˜æº¢å‡º
        - å®æ—¶æ˜¾ç¤ºæ’å…¥è¿›åº¦
        - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆé¿å…é‡å¤æ’å…¥ï¼‰
        - æ›´å¥½çš„æ€§èƒ½å’Œç¨³å®šæ€§
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            delay: æ¯æ¬¡è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
            exchange: äº¤æ˜“æ‰€
            db_instance: æ•°æ®åº“å®ä¾‹
            batch_days: æ¯æ‰¹å¤„ç†çš„äº¤æ˜“æ—¥æ•°é‡
            
        Returns:
            dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        import time
        
        if db_instance is None:
            logger.error("éœ€è¦æä¾›æ•°æ®åº“å®ä¾‹è¿›è¡Œåˆ†æ‰¹æ’å…¥")
            return {}
        
        # è·å–äº¤æ˜“æ—¥å†
        trade_cal = self.get_trade_calendar(start_date, end_date, exchange)
        if trade_cal is None or trade_cal.empty:
            logger.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œé€€å‡ºæ•°æ®è·å–")
            return {}
        
        trading_days = trade_cal['cal_date'].values
        total_days = len(trading_days)
        logger.info(f"ğŸš€ å¼€å§‹å…¨å¸‚åœºæ•°æ®è·å–å’Œåˆ†æ‰¹æ’å…¥ï¼Œå…± {total_days} ä¸ªäº¤æ˜“æ—¥")
        logger.info(f"ğŸ“¦ åˆ†æ‰¹è®¾ç½®: æ¯ {batch_days} ä¸ªäº¤æ˜“æ—¥æ’å…¥ä¸€æ¬¡æ•°æ®åº“")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_trading_days': total_days,
            'successful_days': 0,
            'total_records': 0,
            'total_batches': 0,
            'failed_days': [],
            'batch_insert_success': 0,
            'batch_insert_failed': 0
        }
        
        current_batch_data = []
        batch_trading_days = []
        
        for i, trade_date in enumerate(trading_days, 1):
            try:
                logger.info(f"ğŸ“… æ­£åœ¨è·å– {trade_date} çš„å…¨å¸‚åœºæ•°æ® ({i}/{total_days})")
                
                # ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–æ•°æ®
                df = self.get_daily_with_retry(trade_date=trade_date)
                
                if df is not None and not df.empty:
                    current_batch_data.append(df)
                    batch_trading_days.append(trade_date)
                    stats['successful_days'] += 1
                    logger.info(f"âœ… æˆåŠŸè·å– {trade_date} çš„ {len(df)} åªè‚¡ç¥¨æ•°æ®")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {trade_date} çš„æ•°æ®")
                    stats['failed_days'].append(trade_date)
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(delay)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’å…¥æ•°æ®åº“
                should_insert = (
                    len(current_batch_data) >= batch_days or  # è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                    i == total_days or  # æ˜¯æœ€åä¸€ä¸ªäº¤æ˜“æ—¥
                    len(current_batch_data) >= 20  # æ•°æ®é‡è¾ƒå¤§æ—¶æå‰æ’å…¥
                )
                
                if should_insert and current_batch_data:
                    # åˆå¹¶å½“å‰æ‰¹æ¬¡æ•°æ®
                    batch_df = pd.concat(current_batch_data, ignore_index=True)
                    batch_records = len(batch_df)
                    
                    logger.info(f"ğŸ’¾ å¼€å§‹æ’å…¥ç¬¬ {stats['total_batches'] + 1} æ‰¹æ•°æ®...")
                    logger.info(f"   ğŸ“Š æœ¬æ‰¹æ•°æ®: {batch_records:,} æ¡è®°å½•")
                    logger.info(f"   ğŸ“… äº¤æ˜“æ—¥: {batch_trading_days[0]} åˆ° {batch_trading_days[-1]}")
                    
                    # æ’å…¥æ•°æ®åº“
                    insert_success = db_instance.insert_daily_data(batch_df)
                    
                    if insert_success:
                        stats['total_batches'] += 1
                        stats['total_records'] += batch_records
                        stats['batch_insert_success'] += 1
                        logger.info(f"âœ… ç¬¬ {stats['total_batches']} æ‰¹æ•°æ®æ’å…¥æˆåŠŸï¼")
                        logger.info(f"   ğŸ“ˆ ç´¯è®¡æ’å…¥: {stats['total_records']:,} æ¡è®°å½•")
                    else:
                        stats['batch_insert_failed'] += 1
                        logger.error(f"âŒ ç¬¬ {stats['total_batches'] + 1} æ‰¹æ•°æ®æ’å…¥å¤±è´¥")
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
                    current_batch_data = []
                    batch_trading_days = []
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_days:
                    success_rate = stats['successful_days'] / i * 100
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_days} ({i/total_days*100:.1f}%), "
                              f"æˆåŠŸè·å–: {stats['successful_days']}å¤© ({success_rate:.1f}%)")
                    logger.info(f"   ğŸ’¾ å·²æ’å…¥: {stats['total_records']:,} æ¡è®°å½•")
                
            except Exception as e:
                logger.error(f"âŒ è·å– {trade_date} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                stats['failed_days'].append(trade_date)
                continue
        
        # æœ€ç»ˆç»Ÿè®¡
        logger.info(f"ğŸ‰ å…¨å¸‚åœºæ•°æ®è·å–å’Œæ’å…¥å®Œæˆï¼")
        logger.info(f"   ğŸ“… æ€»äº¤æ˜“æ—¥: {stats['total_trading_days']} å¤©")
        logger.info(f"   âœ… æˆåŠŸè·å–: {stats['successful_days']} å¤©")
        logger.info(f"   ğŸ“Š æ€»æ’å…¥è®°å½•: {stats['total_records']:,} æ¡")
        logger.info(f"   ğŸ“¦ æ’å…¥æ‰¹æ¬¡: {stats['total_batches']} æ¬¡")
        logger.info(f"   ğŸ’¾ æ’å…¥æˆåŠŸç‡: {stats['batch_insert_success']}/{stats['total_batches']}")
        
        if stats['failed_days']:
            logger.warning(f"   âš ï¸ å¤±è´¥çš„äº¤æ˜“æ—¥: {len(stats['failed_days'])} å¤©")
            logger.debug(f"   å¤±è´¥æ—¥æœŸ: {stats['failed_days']}")
        
        return stats
    
    def estimate_market_data_time(self, start_date: str, end_date: str, delay: float = 0.5) -> str:
        """
        é¢„ä¼°å…¨å¸‚åœºæ•°æ®è·å–æ—¶é—´
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ  
            delay: å»¶è¿Ÿæ—¶é—´
            
        Returns:
            str: é¢„ä¼°æ—¶é—´æè¿°
        """
        # ç²—ç•¥ä¼°ç®—äº¤æ˜“æ—¥æ•°é‡ï¼ˆæ¯å¹´çº¦220ä¸ªäº¤æ˜“æ—¥ï¼‰
        from datetime import datetime
        try:
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            days_diff = (end_dt - start_dt).days
            estimated_trading_days = int(days_diff * 220 / 365)  # ç²—ç•¥ä¼°ç®—
        except:
            estimated_trading_days = 220  # é»˜è®¤ä¸€å¹´
        
        total_seconds = estimated_trading_days * delay
        
        if total_seconds < 60:
            return f"{total_seconds:.0f}ç§’"
        elif total_seconds < 3600:
            return f"{total_seconds/60:.1f}åˆ†é’Ÿ"
        else:
            return f"{total_seconds/3600:.1f}å°æ—¶"
    
    def get_ths_index(self, ts_code: str = None, exchange: str = None, 
                     index_type: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®
        
        æ ¹æ®Tushareæ–‡æ¡£ï¼Œéœ€è¦5000ç§¯åˆ†æƒé™ï¼Œå•æ¬¡æœ€å¤§è¿”å›5000è¡Œæ•°æ®
        
        Args:
            ts_code: æŒ‡æ•°ä»£ç 
            exchange: å¸‚åœºç±»å‹ A-aè‚¡ HK-æ¸¯è‚¡ US-ç¾è‚¡
            index_type: æŒ‡æ•°ç±»å‹ N-æ¦‚å¿µæŒ‡æ•° I-è¡Œä¸šæŒ‡æ•° R-åœ°åŸŸæŒ‡æ•° S-åŒèŠ±é¡ºç‰¹è‰²æŒ‡æ•° 
                       ST-åŒèŠ±é¡ºé£æ ¼æŒ‡æ•° TH-åŒèŠ±é¡ºä¸»é¢˜æŒ‡æ•° BB-åŒèŠ±é¡ºå®½åŸºæŒ‡æ•°
            
        Returns:
            pd.DataFrame: åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®
        """
        try:
            logger.info("æ­£åœ¨è·å–åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®...")
            
            # æ„å»ºå‚æ•°å­—å…¸
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if exchange:
                params['exchange'] = exchange
            if index_type:
                params['type'] = index_type
            
            # è°ƒç”¨Tushare API
            df = self.pro.ths_index(**params)
            
            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            if 'list_date' in df.columns:
                # å°†list_dateè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                df['list_date'] = pd.to_datetime(df['list_date'], format='%Y%m%d', errors='coerce')
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if 'type' in df.columns:
                type_counts = df['type'].value_counts()
                logger.info("æŒ‡æ•°ç±»å‹åˆ†å¸ƒï¼š")
                for idx_type, count in type_counts.items():
                    type_name = self._get_index_type_name(idx_type)
                    logger.info(f"  {type_name}({idx_type}): {count}ä¸ª")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæŒ‡æ•°ä¿¡æ¯
            logger.info("éƒ¨åˆ†æŒ‡æ•°ç¤ºä¾‹ï¼š")
            for i, (_, row) in enumerate(df.head(3).iterrows()):
                type_name = self._get_index_type_name(row.get('type', ''))
                logger.info(f"  {row.get('name', 'N/A')}({row.get('ts_code', 'N/A')}) - {type_name} - æˆåˆ†è‚¡:{row.get('count', 'N/A')}ä¸ª")
                
            return df
            
        except Exception as e:
            logger.error(f"è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é—®é¢˜
            if "æƒé™" in str(e) or "ç§¯åˆ†" in str(e) or "permission" in str(e).lower():
                logger.error("å¯èƒ½æ˜¯æƒé™ä¸è¶³ï¼Œéœ€è¦5000ç§¯åˆ†æ‰èƒ½è°ƒç”¨ths_indexæ¥å£")
                logger.info("è¯·æ£€æŸ¥æ‚¨çš„Tushareè´¦æˆ·ç§¯åˆ†æˆ–å‡çº§è´¦æˆ·æƒé™")
            
            return None
    
    def _get_index_type_name(self, index_type: str) -> str:
        """
        è·å–æŒ‡æ•°ç±»å‹ä¸­æ–‡åç§°
        
        Args:
            index_type: æŒ‡æ•°ç±»å‹ä»£ç 
            
        Returns:
            str: ä¸­æ–‡åç§°
        """
        type_mapping = {
            'N': 'æ¦‚å¿µæŒ‡æ•°',
            'I': 'è¡Œä¸šæŒ‡æ•°', 
            'R': 'åœ°åŸŸæŒ‡æ•°',
            'S': 'åŒèŠ±é¡ºç‰¹è‰²æŒ‡æ•°',
            'ST': 'åŒèŠ±é¡ºé£æ ¼æŒ‡æ•°',
            'TH': 'åŒèŠ±é¡ºä¸»é¢˜æŒ‡æ•°',
            'BB': 'åŒèŠ±é¡ºå®½åŸºæŒ‡æ•°'
        }
        return type_mapping.get(index_type, 'æœªçŸ¥ç±»å‹')
    
    def get_all_ths_index_data(self) -> Optional[pd.DataFrame]:
        """
        è·å–æ‰€æœ‰åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®ï¼ˆåˆ†ç±»å‹è·å–ï¼‰
        
        ç”±äºAPIå•æ¬¡è°ƒç”¨é™åˆ¶5000æ¡ï¼Œè¿™é‡Œåˆ†ç±»å‹è·å–ä»¥ç¡®ä¿è·å–å®Œæ•´æ•°æ®
        
        Returns:
            pd.DataFrame: æ‰€æœ‰æŒ‡æ•°æ•°æ®
        """
        logger.info("ğŸš€ å¼€å§‹è·å–æ‰€æœ‰åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®...")
        
        # å®šä¹‰è¦è·å–çš„æŒ‡æ•°ç±»å‹
        index_types = ['N', 'I', 'R', 'S', 'ST', 'TH', 'BB']
        all_data = []
        
        for index_type in index_types:
            try:
                type_name = self._get_index_type_name(index_type)
                logger.info(f"æ­£åœ¨è·å–{type_name}({index_type})...")
                
                df = self.get_ths_index(index_type=index_type)
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    logger.info(f"âœ… æˆåŠŸè·å–{type_name} {len(df)} ä¸ªæŒ‡æ•°")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°{type_name}æ•°æ®")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                import time
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"âŒ è·å–{type_name}æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®")
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        
        logger.info(f"ğŸ‰ åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»æŒ‡æ•°æ•°é‡: {len(combined_df)} ä¸ª")
        
        # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        if 'type' in combined_df.columns:
            type_summary = combined_df['type'].value_counts()
            logger.info("ğŸ“ˆ æŒ‡æ•°ç±»å‹æ±‡æ€»ï¼š")
            for idx_type, count in type_summary.items():
                type_name = self._get_index_type_name(idx_type)
                logger.info(f"   {type_name}: {count} ä¸ª")
        
        return combined_df
    
    def get_ths_member(self, ts_code: str = None, con_code: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
        
        æ ¹æ®Tushareæ–‡æ¡£ï¼Œéœ€è¦5000ç§¯åˆ†æƒé™ï¼Œæ¯åˆ†é’Ÿå¯è°ƒå–200æ¬¡
        
        Args:
            ts_code: æ¿å—æŒ‡æ•°ä»£ç 
            con_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            pd.DataFrame: æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®...")
            
            # æ„å»ºå‚æ•°å­—å…¸
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if con_code:
                params['con_code'] = con_code
            
            # è°ƒç”¨Tushare API
            df = self.pro.ths_member(**params)
            
            if df is None or df.empty:
                logger.warning(f"æœªè·å–åˆ°æŒ‡æ•° {ts_code} çš„æˆåˆ†è‚¡æ•°æ®")
                return None
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡æˆåˆ†è‚¡æ•°æ®")
            
            # æ˜¾ç¤ºæˆåˆ†è‚¡ä¿¡æ¯
            if len(df) > 0:
                logger.info(f"æˆåˆ†è‚¡ç¤ºä¾‹ï¼š")
                for i, (_, row) in enumerate(df.head(3).iterrows()):
                    logger.info(f"  {row.get('con_name', 'N/A')}({row.get('con_code', 'N/A')})")
                    
            return df
            
        except Exception as e:
            logger.error(f"è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é—®é¢˜
            if "æƒé™" in str(e) or "ç§¯åˆ†" in str(e) or "permission" in str(e).lower():
                logger.error("å¯èƒ½æ˜¯æƒé™ä¸è¶³ï¼Œéœ€è¦5000ç§¯åˆ†æ‰èƒ½è°ƒç”¨ths_memberæ¥å£")
                logger.info("è¯·æ£€æŸ¥æ‚¨çš„Tushareè´¦æˆ·ç§¯åˆ†æˆ–å‡çº§è´¦æˆ·æƒé™")
            
            return None
    
    def get_all_concept_members(self, concept_indexes: List[str] = None, 
                               batch_delay: float = 0.3) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰æ¦‚å¿µæŒ‡æ•°çš„æˆåˆ†è‚¡æ•°æ®
        
        Args:
            concept_indexes: æ¦‚å¿µæŒ‡æ•°ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä»æ•°æ®åº“ä¸­è·å–
            batch_delay: æ¯æ¬¡APIè°ƒç”¨çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è§¦å‘é¢‘ç‡é™åˆ¶
            
        Returns:
            pd.DataFrame: æ‰€æœ‰æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
        """
        import time
        
        logger.info("ğŸš€ å¼€å§‹è·å–æ‰€æœ‰æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®...")
        
        # å¦‚æœæ²¡æœ‰æä¾›æŒ‡æ•°åˆ—è¡¨ï¼Œä»æ•°æ®åº“è·å–æ¦‚å¿µæŒ‡æ•°
        if concept_indexes is None:
            try:
                from database import StockDatabase
                with StockDatabase() as db:
                    # åªè·å–æ¦‚å¿µæŒ‡æ•°(N)
                    concept_df = db.query_ths_index(index_type='N')
                    if concept_df is not None and not concept_df.empty:
                        concept_indexes = concept_df['ts_code'].tolist()
                        logger.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(concept_indexes)} ä¸ªæ¦‚å¿µæŒ‡æ•°")
                    else:
                        logger.error("æ•°æ®åº“ä¸­æ²¡æœ‰æ¦‚å¿µæŒ‡æ•°æ•°æ®")
                        return pd.DataFrame()
            except Exception as e:
                logger.error(f"ä»æ•°æ®åº“è·å–æ¦‚å¿µæŒ‡æ•°å¤±è´¥: {e}")
                return pd.DataFrame()
        
        if not concept_indexes:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¦‚å¿µæŒ‡æ•°åˆ—è¡¨")
            return pd.DataFrame()
        
        all_members_data = []
        total_indexes = len(concept_indexes)
        successful_count = 0
        failed_count = 0
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {total_indexes} ä¸ªæ¦‚å¿µæŒ‡æ•°çš„æˆåˆ†è‚¡æ•°æ®")
        
        for i, ts_code in enumerate(concept_indexes, 1):
            try:
                logger.info(f"æ­£åœ¨è·å–æŒ‡æ•° {ts_code} çš„æˆåˆ†è‚¡ ({i}/{total_indexes})")
                
                # è·å–å•ä¸ªæŒ‡æ•°çš„æˆåˆ†è‚¡
                df = self.get_ths_member(ts_code=ts_code)
                
                if df is not None and not df.empty:
                    all_members_data.append(df)
                    successful_count += 1
                    logger.info(f"âœ… æˆåŠŸè·å– {ts_code} çš„ {len(df)} åªæˆåˆ†è‚¡")
                else:
                    failed_count += 1
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„æˆåˆ†è‚¡æ•°æ®")
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_indexes:
                    success_rate = successful_count / i * 100
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_indexes} ({i/total_indexes*100:.1f}%), "
                              f"æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count} ({success_rate:.1f}%)")
                
                # APIè°ƒç”¨å»¶è¿Ÿï¼Œé˜²æ­¢é¢‘ç‡é™åˆ¶
                time.sleep(batch_delay)
                
            except Exception as e:
                failed_count += 1
                logger.error(f"âŒ è·å– {ts_code} æˆåˆ†è‚¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_members_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_members_data, ignore_index=True)
        
        logger.info(f"ğŸ‰ æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»æˆåˆ†è‚¡è®°å½•: {len(combined_df):,} æ¡")
        logger.info(f"   ğŸ“ˆ æ¶‰åŠæŒ‡æ•°: {successful_count} ä¸ª")
        logger.info(f"   ğŸ“ˆ ä¸é‡å¤è‚¡ç¥¨: {combined_df['con_code'].nunique() if 'con_code' in combined_df.columns else 0} åª")
        logger.info(f"   âœ… æˆåŠŸç‡: {successful_count}/{total_indexes} ({successful_count/total_indexes*100:.1f}%)")
        
        return combined_df
    
    def get_concept_members_batch_with_db_insert(self, db_instance=None, 
                                               concept_indexes: List[str] = None,
                                               batch_delay: float = 0.3,
                                               batch_size: int = 20) -> dict:
        """
        æ‰¹é‡è·å–æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡å¹¶åˆ†æ‰¹æ’å…¥æ•°æ®åº“
        
        Args:
            db_instance: æ•°æ®åº“å®ä¾‹
            concept_indexes: æ¦‚å¿µæŒ‡æ•°ä»£ç åˆ—è¡¨
            batch_delay: APIè°ƒç”¨å»¶è¿Ÿ
            batch_size: åˆ†æ‰¹æ’å…¥çš„æ•°é‡
            
        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        import time
        
        if db_instance is None:
            logger.error("éœ€è¦æä¾›æ•°æ®åº“å®ä¾‹è¿›è¡Œåˆ†æ‰¹æ’å…¥")
            return {}
        
        # å¦‚æœæ²¡æœ‰æä¾›æŒ‡æ•°åˆ—è¡¨ï¼Œä»æ•°æ®åº“è·å–æ¦‚å¿µæŒ‡æ•°
        if concept_indexes is None:
            concept_df = db_instance.query_ths_index(index_type='N')
            if concept_df is not None and not concept_df.empty:
                concept_indexes = concept_df['ts_code'].tolist()
                logger.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(concept_indexes)} ä¸ªæ¦‚å¿µæŒ‡æ•°")
            else:
                logger.error("æ•°æ®åº“ä¸­æ²¡æœ‰æ¦‚å¿µæŒ‡æ•°æ•°æ®")
                return {}
        
        if not concept_indexes:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¦‚å¿µæŒ‡æ•°åˆ—è¡¨")
            return {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_indexes': len(concept_indexes),
            'successful_indexes': 0,
            'failed_indexes': 0,
            'total_members': 0,
            'batch_count': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'failed_index_codes': []
        }
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è·å–å¹¶æ’å…¥ {stats['total_indexes']} ä¸ªæ¦‚å¿µæŒ‡æ•°çš„æˆåˆ†è‚¡æ•°æ®")
        logger.info(f"ğŸ“¦ åˆ†æ‰¹è®¾ç½®: æ¯ {batch_size} ä¸ªæŒ‡æ•°æ’å…¥ä¸€æ¬¡æ•°æ®åº“")
        
        current_batch_data = []
        
        for i, ts_code in enumerate(concept_indexes, 1):
            try:
                logger.info(f"ğŸ“Š æ­£åœ¨è·å–æŒ‡æ•° {ts_code} çš„æˆåˆ†è‚¡ ({i}/{stats['total_indexes']})")
                
                # è·å–å•ä¸ªæŒ‡æ•°çš„æˆåˆ†è‚¡
                df = self.get_ths_member(ts_code=ts_code)
                
                if df is not None and not df.empty:
                    current_batch_data.append(df)
                    stats['successful_indexes'] += 1
                    stats['total_members'] += len(df)
                    logger.info(f"âœ… æˆåŠŸè·å– {ts_code} çš„ {len(df)} åªæˆåˆ†è‚¡")
                else:
                    stats['failed_indexes'] += 1
                    stats['failed_index_codes'].append(ts_code)
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„æˆåˆ†è‚¡æ•°æ®")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(batch_delay)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’å…¥æ•°æ®åº“
                should_insert = (
                    len(current_batch_data) >= batch_size or  # è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                    i == stats['total_indexes']  # æ˜¯æœ€åä¸€ä¸ªæŒ‡æ•°
                )
                
                if should_insert and current_batch_data:
                    # åˆå¹¶å½“å‰æ‰¹æ¬¡æ•°æ®
                    batch_df = pd.concat(current_batch_data, ignore_index=True)
                    batch_records = len(batch_df)
                    
                    logger.info(f"ğŸ’¾ å¼€å§‹æ’å…¥ç¬¬ {stats['batch_count'] + 1} æ‰¹æ•°æ®...")
                    logger.info(f"   ğŸ“Š æœ¬æ‰¹æ•°æ®: {batch_records:,} æ¡æˆåˆ†è‚¡è®°å½•")
                    
                    # æ’å…¥æ•°æ®åº“
                    insert_success = db_instance.insert_ths_member(batch_df)
                    
                    if insert_success:
                        stats['batch_count'] += 1
                        stats['successful_batches'] += 1
                        logger.info(f"âœ… ç¬¬ {stats['batch_count']} æ‰¹æ•°æ®æ’å…¥æˆåŠŸï¼")
                    else:
                        stats['failed_batches'] += 1
                        logger.error(f"âŒ ç¬¬ {stats['batch_count'] + 1} æ‰¹æ•°æ®æ’å…¥å¤±è´¥")
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
                    current_batch_data = []
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == stats['total_indexes']:
                    success_rate = stats['successful_indexes'] / i * 100
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{stats['total_indexes']} ({i/stats['total_indexes']*100:.1f}%), "
                              f"æˆåŠŸ: {stats['successful_indexes']}, å¤±è´¥: {stats['failed_indexes']} ({success_rate:.1f}%)")
                    logger.info(f"   ğŸ’¾ å·²æ’å…¥: {stats['total_members']:,} æ¡æˆåˆ†è‚¡è®°å½•")
                
            except Exception as e:
                stats['failed_indexes'] += 1
                stats['failed_index_codes'].append(ts_code)
                logger.error(f"âŒ è·å– {ts_code} æˆåˆ†è‚¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        # æœ€ç»ˆç»Ÿè®¡
        logger.info(f"ğŸ‰ æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®è·å–å’Œæ’å…¥å®Œæˆï¼")
        logger.info(f"   ğŸ“Š å¤„ç†æŒ‡æ•°: {stats['total_indexes']} ä¸ª")
        logger.info(f"   âœ… æˆåŠŸæŒ‡æ•°: {stats['successful_indexes']} ä¸ª")
        logger.info(f"   ğŸ“Š æ€»æˆåˆ†è‚¡è®°å½•: {stats['total_members']:,} æ¡")
        logger.info(f"   ğŸ“¦ æ’å…¥æ‰¹æ¬¡: {stats['batch_count']} æ¬¡")
        logger.info(f"   ğŸ’¾ æ’å…¥æˆåŠŸç‡: {stats['successful_batches']}/{stats['batch_count']}")
        
        if stats['failed_index_codes']:
            logger.warning(f"   âš ï¸ å¤±è´¥çš„æŒ‡æ•°: {len(stats['failed_index_codes'])} ä¸ª")
            logger.debug(f"   å¤±è´¥æŒ‡æ•°ä»£ç : {stats['failed_index_codes']}")
        
        return stats