# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®è·å–å™¨æ¨¡å—
è´Ÿè´£ä»Tushare APIè·å–è‚¡ç¥¨æ•°æ®
"""

import tushare as ts
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import List, Optional

from config import TUSHARE_TOKEN
from log_config import get_logger

logger = get_logger(__name__)


class StockDataFetcher:
    """è‚¡ç¥¨æ•°æ®è·å–å™¨"""
    
    def __init__(self, token: str = None):
        """
        åˆå§‹åŒ–æ•°æ®è·å–å™¨
        
        Args:
            token: Tushare API token
        """
        self.token = token or TUSHARE_TOKEN
        if not self.token or self.token == "your_tushare_token_here":
            raise ValueError("è¯·åœ¨config.pyä¸­è®¾ç½®æœ‰æ•ˆçš„Tushare token")
        
        # è®¾ç½®tushare token
        ts.set_token(self.token)
        self.pro = ts.pro_api()
        
    def get_daily_data(self, ts_code: str, start_date: str = None, 
                      end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001.SZï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            
        Returns:
            pd.DataFrame: æ—¥çº¿æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å– {ts_code} çš„æ—¥çº¿æ•°æ®...")
            
            # è·å–æ—¥çº¿æ•°æ®
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )
            
            if df.empty:
                logger.warning(f"è‚¡ç¥¨ {ts_code} åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            
            logger.info(f"æˆåŠŸè·å– {ts_code} çš„ {len(df)} æ¡æ—¥çº¿æ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å– {ts_code} æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_weekly_data(self, ts_code: str, start_date: str = None, 
                       end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨çš„å‘¨çº¿æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001.SZï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            
        Returns:
            pd.DataFrame: å‘¨çº¿æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å– {ts_code} çš„å‘¨çº¿æ•°æ®...")
            
            # è·å–å‘¨çº¿æ•°æ®
            df = self.pro.weekly(
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )
            
            if df.empty:
                logger.warning(f"è‚¡ç¥¨ {ts_code} åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰å‘¨çº¿æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            
            logger.info(f"æˆåŠŸè·å– {ts_code} çš„ {len(df)} æ¡å‘¨çº¿æ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å– {ts_code} å‘¨çº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_multiple_stocks_data(self, stock_codes: List[str], 
                                start_date: str = None, end_date: str = None,
                                batch_size: int = 50, delay: float = 0.5) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé˜²æ­¢APIè°ƒç”¨è¿‡äºé¢‘ç¹
            delay: æ¯æ¬¡è°ƒç”¨çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„æ—¥çº¿æ•°æ®
        """
        all_data = []
        total_stocks = len(stock_codes)
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {total_stocks} åªè‚¡ç¥¨æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        for i, ts_code in enumerate(stock_codes, 1):
            try:
                # è·å–å•åªè‚¡ç¥¨æ•°æ®
                df = self.get_daily_data(ts_code, start_date, end_date)
                if df is not None and not df.empty:
                    all_data.append(df)
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_stocks:
                    success_count = len(all_data)
                    logger.info(f"è¿›åº¦: {i}/{total_stocks} ({i/total_stocks*100:.1f}%), æˆåŠŸè·å–: {success_count}åª")
                
                # é¿å…é¢‘ç¹è°ƒç”¨APIï¼Œé€‚å½“ä¼‘çœ 
                import time
                time.sleep(delay)
                
                # æ¯æ‰¹æ¬¡åç¨é•¿ä¼‘çœ 
                if i % batch_size == 0:
                    logger.info(f"å®Œæˆç¬¬ {i//batch_size} æ‰¹æ¬¡ï¼Œä¼‘çœ 2ç§’...")
                    time.sleep(2.0)
                
            except Exception as e:
                logger.error(f"è·å–è‚¡ç¥¨ {ts_code} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.warning("æ²¡æœ‰è·å–åˆ°ä»»ä½•è‚¡ç¥¨æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        success_rate = len(all_data) / total_stocks * 100
        logger.info(f"æ‰¹é‡è·å–å®Œæˆï¼æ€»å…±è·å–äº† {len(combined_df)} æ¡è‚¡ç¥¨æ•°æ®è®°å½•")
        logger.info(f"æˆåŠŸç‡: {len(all_data)}/{total_stocks} ({success_rate:.1f}%)")
        
        return combined_df
    
    def get_multiple_stocks_weekly_data(self, stock_codes: List[str], 
                                       start_date: str = None, end_date: str = None,
                                       batch_size: int = 50, delay: float = 0.5) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨çš„å‘¨çº¿æ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé˜²æ­¢APIè°ƒç”¨è¿‡äºé¢‘ç¹
            delay: æ¯æ¬¡è°ƒç”¨çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„å‘¨çº¿æ•°æ®
        """
        all_data = []
        total_stocks = len(stock_codes)
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {total_stocks} åªè‚¡ç¥¨å‘¨çº¿æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        for i, ts_code in enumerate(stock_codes, 1):
            try:
                # è·å–å•åªè‚¡ç¥¨å‘¨çº¿æ•°æ®
                df = self.get_weekly_data(ts_code, start_date, end_date)
                if df is not None and not df.empty:
                    all_data.append(df)
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_stocks:
                    success_count = len(all_data)
                    logger.info(f"è¿›åº¦: {i}/{total_stocks} ({i/total_stocks*100:.1f}%), æˆåŠŸè·å–: {success_count}åª")
                
                # é¿å…é¢‘ç¹è°ƒç”¨APIï¼Œé€‚å½“ä¼‘çœ 
                import time
                time.sleep(delay)
                
                # æ¯æ‰¹æ¬¡åç¨é•¿ä¼‘çœ 
                if i % batch_size == 0:
                    logger.info(f"å®Œæˆç¬¬ {i//batch_size} æ‰¹æ¬¡ï¼Œä¼‘çœ 2ç§’...")
                    time.sleep(2.0)
                
            except Exception as e:
                logger.error(f"è·å–è‚¡ç¥¨ {ts_code} å‘¨çº¿æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.warning("æ²¡æœ‰è·å–åˆ°ä»»ä½•è‚¡ç¥¨å‘¨çº¿æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        success_rate = len(all_data) / total_stocks * 100
        logger.info(f"æ‰¹é‡è·å–å‘¨çº¿æ•°æ®å®Œæˆï¼æ€»å…±è·å–äº† {len(combined_df)} æ¡å‘¨çº¿æ•°æ®è®°å½•")
        logger.info(f"æˆåŠŸç‡: {len(all_data)}/{total_stocks} ({success_rate:.1f}%)")
        
        return combined_df
    
    def get_daily_by_date(self, trade_date: str, ts_code: str = None) -> Optional[pd.DataFrame]:
        """
        æ ¹æ®äº¤æ˜“æ—¥æœŸè·å–å½“æ—¥æ‰€æœ‰è‚¡ç¥¨æˆ–æŒ‡å®šè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®
        
        è¿™ç§æ–¹å¼é€‚åˆï¼š
        - è·å–æŸä¸ªäº¤æ˜“æ—¥çš„å…¨å¸‚åœºæ•°æ®
        - å¯¹æ¯”æŸä¸ªäº¤æ˜“æ—¥ä¸åŒè‚¡ç¥¨çš„è¡¨ç°
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            ts_code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœæŒ‡å®šåˆ™åªè·å–è¯¥è‚¡ç¥¨æ•°æ®
            
        Returns:
            pd.DataFrame: å½“æ—¥è¡Œæƒ…æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å– {trade_date} çš„è¡Œæƒ…æ•°æ®...")
            
            # ä½¿ç”¨trade_dateå‚æ•°è·å–æ•°æ®
            df = self.pro.daily(
                trade_date=trade_date,
                ts_code=ts_code  # å¯ä»¥ä¸ºNoneè·å–å…¨å¸‚åœºæ•°æ®
            )
            
            if df.empty:
                logger.warning(f"äº¤æ˜“æ—¥ {trade_date} æ²¡æœ‰è¡Œæƒ…æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            
            if ts_code:
                logger.info(f"æˆåŠŸè·å– {ts_code} åœ¨ {trade_date} çš„æ•°æ®")
            else:
                logger.info(f"æˆåŠŸè·å– {trade_date} å…¨å¸‚åœº {len(df)} æ¡æ•°æ®")
            
            return df
            
        except Exception as e:
            logger.error(f"è·å– {trade_date} è¡Œæƒ…æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_latest_trading_day_data(self, stock_codes: List[str] = None) -> Optional[pd.DataFrame]:
        """
        è·å–æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–å…¨å¸‚åœºæ•°æ®
            
        Returns:
            pd.DataFrame: æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        """
        # è·å–æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥è¿›è¡Œå°è¯•
        today = datetime.now()
        for i in range(10):  # å°è¯•æœ€è¿‘10å¤©
            check_date = (today - timedelta(days=i)).strftime('%Y%m%d')
            
            try:
                if stock_codes:
                    # è·å–æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
                    all_data = []
                    for ts_code in stock_codes:
                        df = self.get_daily_by_date(check_date, ts_code)
                        if df is not None and not df.empty:
                            all_data.append(df)
                    
                    if all_data:
                        return pd.concat(all_data, ignore_index=True)
                else:
                    # è·å–å…¨å¸‚åœºæ•°æ®
                    return self.get_daily_by_date(check_date)
                    
            except:
                continue
        
        logger.warning("æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®")
        return None

    def get_stock_basic(self, exchange: str = None, is_hs: str = None, 
                       list_status: str = 'L', market: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        
        Args:
            exchange: äº¤æ˜“æ‰€ï¼ˆSSEä¸Šäº¤æ‰€ SZSEæ·±äº¤æ‰€ï¼‰
            is_hs: æ˜¯å¦æ²ªæ·±æ¸¯é€šæ ‡çš„ï¼ˆNå¦ Hæ²ªè‚¡é€š Sæ·±è‚¡é€šï¼‰
            list_status: ä¸Šå¸‚çŠ¶æ€ Lä¸Šå¸‚ Dé€€å¸‚ Pæš‚åœä¸Šå¸‚ï¼ˆé»˜è®¤Lï¼‰
            market: å¸‚åœºç±»å‹ ä¸»æ¿Main åˆ›ä¸šæ¿ChiNext ç§‘åˆ›æ¿STARï¼ˆé»˜è®¤Noneè·å–æ‰€æœ‰ï¼‰
            
        Returns:
            pd.DataFrame: è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        """
        try:
            logger.info("æ­£åœ¨è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
            df = self.pro.stock_basic(
                exchange=exchange, 
                is_hs=is_hs,
                list_status=list_status,
                market=market,
                fields='ts_code,symbol,name,area,industry,market,list_date,list_status'
            )
            logger.info(f"è·å–åˆ° {len(df)} åªè‚¡ç¥¨çš„åŸºç¡€ä¿¡æ¯")
            return df
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def get_etf_basic(self,
                      list_status: str = 'L',
                      exchange: str = None,
                      mgr: str = None,
                      index_code: str = None,
                      etf_type: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–ETFåŸºç¡€ä¿¡æ¯

        å¯¹åº”Tushare etf_basicæ¥å£æ–‡æ¡£:
        https://tushare.pro/document/2?doc_id=385

        Args:
            list_status: ä¸Šå¸‚çŠ¶æ€ L-ä¸Šå¸‚ D-é€€å¸‚ P-æš‚åœä¸Šå¸‚
            exchange: äº¤æ˜“æ‰€ (SSE, SZSE)
            mgr: åŸºé‡‘ç®¡ç†äººåç§°ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
            index_code: è·Ÿè¸ªæŒ‡æ•°ä»£ç 
            etf_type: ETFç±»å‹

        Returns:
            pd.DataFrame: ETFåŸºç¡€ä¿¡æ¯
        """
        try:
            logger.info("æ­£åœ¨è·å–ETFåŸºç¡€ä¿¡æ¯...")

            params = {
                "list_status": list_status,
            }
            if exchange:
                params["exchange"] = exchange
            if mgr:
                params["mgr"] = mgr
            if index_code:
                params["index_code"] = index_code
            if etf_type:
                params["etf_type"] = etf_type

            # é€‰å–å¸¸ç”¨å­—æ®µï¼Œæ–¹ä¾¿è½åº“ï¼›æ–‡æ¡£ç¤ºä¾‹å­—æ®µè§: https://tushare.pro/document/2?doc_id=385
            fields = "ts_code,extname,index_code,index_name,exchange,etf_type,list_date,list_status,delist_date,mgr_name"

            df = self.pro.etf_basic(fields=fields, **params)

            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°ETFåŸºç¡€ä¿¡æ¯æ•°æ®")
                return None

            # æ—¥æœŸå­—æ®µé¢„å¤„ç†
            for col in ["list_date", "delist_date"]:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], format="%Y%m%d", errors="coerce")

            logger.info(f"è·å–åˆ° {len(df)} åªETFçš„åŸºç¡€ä¿¡æ¯")
            return df

        except Exception as e:
            logger.error(f"è·å–ETFåŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def get_main_board_stocks(self, use_cache: bool = True) -> List[str]:
        """
        è·å–Aè‚¡ä¸»æ¿è‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ–‡ä»¶
            
        Returns:
            List[str]: ä¸»æ¿è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        cache_file = 'main_board_stocks_cache.txt'
        
        # å°è¯•ä»ç¼“å­˜æ–‡ä»¶è¯»å–
        if use_cache:
            try:
                import os
                
                if os.path.exists(cache_file):
                    # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(cache_file))
                    if datetime.now() - file_mtime < timedelta(days=7):  # 7å¤©å†…çš„ç¼“å­˜æœ‰æ•ˆ
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            cached_stocks = [line.strip() for line in f.readlines() if line.strip()]
                        logger.info(f"ä»ç¼“å­˜æ–‡ä»¶è¯»å–åˆ° {len(cached_stocks)} åªä¸»æ¿è‚¡ç¥¨")
                        return cached_stocks
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        
        try:
            logger.info("æ­£åœ¨ä»APIè·å–Aè‚¡ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨...")
            
            # è·å–ä¸»æ¿è‚¡ç¥¨ï¼ˆæ’é™¤åˆ›ä¸šæ¿å’Œç§‘åˆ›æ¿ï¼‰
            df = self.pro.stock_basic(
                list_status='L',  # åªè¦ä¸Šå¸‚çš„è‚¡ç¥¨
                fields='ts_code,symbol,name,market,list_date'
            )
            
            if df is not None and not df.empty:
                # è¿‡æ»¤ä¸»æ¿è‚¡ç¥¨ï¼ˆæ’é™¤åˆ›ä¸šæ¿300å¼€å¤´ã€ç§‘åˆ›æ¿688å¼€å¤´ã€åŒ—äº¤æ‰€830/430å¼€å¤´ç­‰ï¼‰
                main_board_df = df[
                    (~df['ts_code'].str.startswith('300')) &  # æ’é™¤åˆ›ä¸šæ¿
                    (~df['ts_code'].str.startswith('688')) &  # æ’é™¤ç§‘åˆ›æ¿
                    (~df['ts_code'].str.startswith('830')) &  # æ’é™¤åŒ—äº¤æ‰€
                    (~df['ts_code'].str.startswith('430')) &  # æ’é™¤åŒ—äº¤æ‰€
                    (~df['ts_code'].str.startswith('200')) &  # æ’é™¤Bè‚¡
                    (~df['ts_code'].str.startswith('900'))    # æ’é™¤Bè‚¡
                ]
                
                stock_codes = main_board_df['ts_code'].tolist()
                logger.info(f"ä»APIè·å–åˆ° {len(stock_codes)} åªAè‚¡ä¸»æ¿è‚¡ç¥¨")
                
                # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        for code in stock_codes:
                            f.write(f"{code}\\n")
                    logger.info("è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶")
                except Exception as e:
                    logger.warning(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
                
                # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
                sh_count = len([code for code in stock_codes if code.endswith('.SH')])
                sz_count = len([code for code in stock_codes if code.endswith('.SZ')])
                logger.info(f"å…¶ä¸­ä¸Šäº¤æ‰€ä¸»æ¿: {sh_count}åª, æ·±äº¤æ‰€ä¸»æ¿: {sz_count}åª")
                
                return stock_codes
            else:
                logger.warning("æœªè·å–åˆ°è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
                return self._get_backup_main_board_stocks()
                
        except Exception as e:
            logger.error(f"è·å–ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            logger.info("ä½¿ç”¨å¤‡ç”¨ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨...")
            return self._get_backup_main_board_stocks()
    
    def _get_backup_main_board_stocks(self) -> List[str]:
        """
        å¤‡ç”¨çš„ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¸¸è§çš„å¤§ç›˜è‚¡ï¼‰
        å½“APIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨
        
        Returns:
            List[str]: å¤‡ç”¨è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        backup_stocks = [
            # æ²ªå¸‚ä¸»æ¿å¤§ç›˜è‚¡
            '600000.SH', '600036.SH', '600519.SH', '600887.SH', '601318.SH',
            '601398.SH', '601857.SH', '601988.SH', '600028.SH', '600030.SH',
            '600050.SH', '600104.SH', '600276.SH', '600690.SH', '600703.SH',
            '600837.SH', '600900.SH', '601012.SH', '601066.SH', '601166.SH',
            '601169.SH', '601229.SH', '601288.SH', '601328.SH', '601336.SH',
            '601390.SH', '601601.SH', '601628.SH', '601668.SH', '601688.SH',
            '601766.SH', '601788.SH', '601818.SH', '601828.SH', '601888.SH',
            '601898.SH', '601919.SH', '601939.SH', '601985.SH', '601989.SH',
            
            # æ·±å¸‚ä¸»æ¿å¤§ç›˜è‚¡
            '000001.SZ', '000002.SZ', '000063.SZ', '000100.SZ', '000157.SZ',
            '000166.SZ', '000333.SZ', '000338.SZ', '000858.SZ', '000895.SZ',
            '000938.SZ', '000961.SZ', '001979.SZ', '002001.SZ', '002007.SZ',
            '002024.SZ', '002027.SZ', '002032.SZ', '002142.SZ', '002202.SZ',
            '002230.SZ', '002236.SZ', '002241.SZ', '002304.SZ', '002352.SZ',
            '002415.SZ', '002456.SZ', '002475.SZ', '002493.SZ', '002508.SZ',
            '002594.SZ', '002601.SZ', '002602.SZ', '002714.SZ', '002736.SZ',
            '002791.SZ', '002812.SZ', '002841.SZ', '002867.SZ', '002916.SZ',
        ]
        
        logger.info(f"ä½¿ç”¨å¤‡ç”¨è‚¡ç¥¨åˆ—è¡¨ï¼ŒåŒ…å« {len(backup_stocks)} åªä¸»è¦çš„ä¸»æ¿è‚¡ç¥¨")
        return backup_stocks
    
    def get_trade_calendar(self, start_date: str, end_date: str, exchange: str = 'SSE') -> Optional[pd.DataFrame]:
        """
        è·å–äº¤æ˜“æ—¥å†
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            exchange: äº¤æ˜“æ‰€ï¼ˆSSEä¸Šäº¤æ‰€ SZSEæ·±äº¤æ‰€ï¼‰
            
        Returns:
            pd.DataFrame: äº¤æ˜“æ—¥å†æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å– {start_date} åˆ° {end_date} çš„äº¤æ˜“æ—¥å†...")
            
            df = self.pro.trade_cal(
                exchange=exchange,
                is_open='1',  # åªè·å–äº¤æ˜“æ—¥
                start_date=start_date,
                end_date=end_date,
                fields='cal_date'
            )
            
            if df is not None and not df.empty:
                logger.info(f"è·å–åˆ° {len(df)} ä¸ªäº¤æ˜“æ—¥")
                return df
            else:
                logger.warning("æœªè·å–åˆ°äº¤æ˜“æ—¥å†æ•°æ®")
                return None
                
        except Exception as e:
            logger.error(f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            return None

    def get_etf_daily(self, ts_code: str = '', trade_date: str = '',
                      start_date: str = '', end_date: str = '') -> Optional[pd.DataFrame]:
        """
        è·å–ETFæ—¥çº¿è¡Œæƒ…æ•°æ®ï¼Œå¯¹åº”Tushare fund_dailyæ¥å£
        æ–‡æ¡£: https://tushare.pro/document/2?doc_id=127

        Args:
            ts_code: ETFä»£ç 
            trade_date: å•ä¸ªäº¤æ˜“æ—¥ (YYYYMMDD)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            pd.DataFrame: ETFæ—¥çº¿æ•°æ®
        """
        try:
            params = {}
            if ts_code:
                params["ts_code"] = ts_code
            if trade_date:
                params["trade_date"] = trade_date
            if start_date:
                params["start_date"] = start_date
            if end_date:
                params["end_date"] = end_date

            df = self.pro.fund_daily(**params)

            if df is None or df.empty:
                return None

            # trade_date å­—æ®µè½¬ä¸ºæ—¥æœŸ
            if "trade_date" in df.columns:
                df["trade_date"] = pd.to_datetime(df["trade_date"], format="%Y%m%d", errors="coerce")

            return df

        except Exception as e:
            logger.error(f"è·å–ETFæ—¥çº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_daily_with_retry(self, ts_code: str = '', trade_date: str = '', 
                           start_date: str = '', end_date: str = '', max_retries: int = 3) -> Optional[pd.DataFrame]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ—¥çº¿æ•°æ®è·å–
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            trade_date: äº¤æ˜“æ—¥æœŸ
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            pd.DataFrame: æ—¥çº¿æ•°æ®
        """
        import time
        
        for retry in range(max_retries):
            try:
                if trade_date:
                    df = self.pro.daily(ts_code=ts_code, trade_date=trade_date)
                else:
                    df = self.pro.daily(ts_code=ts_code, start_date=start_date, end_date=end_date)
                
                if df is not None:
                    # æ•°æ®é¢„å¤„ç†
                    if not df.empty:
                        df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
                    return df
                    
            except Exception as e:
                logger.warning(f"ç¬¬ {retry + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if retry < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•
                    time.sleep(1 + retry)  # é€’å¢å»¶è¿Ÿ
                else:
                    logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œè·å–å¤±è´¥")
        
        return None

    def get_etf_daily_with_retry(self, ts_code: str = '', trade_date: str = '',
                                 start_date: str = '', end_date: str = '',
                                 max_retries: int = 3) -> Optional[pd.DataFrame]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„ETFæ—¥çº¿æ•°æ®è·å–ï¼ˆfund_dailyï¼‰
        """
        import time

        for retry in range(max_retries):
            try:
                df = self.get_etf_daily(
                    ts_code=ts_code,
                    trade_date=trade_date,
                    start_date=start_date,
                    end_date=end_date,
                )
                if df is not None:
                    return df
            except Exception as e:
                logger.warning(f"è·å–ETFæ—¥çº¿ç¬¬ {retry + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if retry < max_retries - 1:
                    time.sleep(1 + retry)
                else:
                    logger.error(f"ETFæ—¥çº¿è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œè·å–å¤±è´¥")

        return None
    
    def get_all_market_data_by_dates(self, start_date: str, end_date: str, 
                                   delay: float = 0.5, exchange: str = 'SSE') -> pd.DataFrame:
        """
        é€šè¿‡äº¤æ˜“æ—¥å¾ªç¯è·å–å…¨å¸‚åœºå†å²æ•°æ®ï¼ˆæ¨èç”¨äºå¤§æ‰¹é‡æ•°æ®è·å–ï¼‰
        
        è¿™ç§æ–¹å¼çš„ä¼˜åŠ¿ï¼š
        - è‚¡ç¥¨æœ‰5000+åªï¼Œæ¯å¹´äº¤æ˜“æ—¥åªæœ‰220å·¦å³ï¼Œå¾ªç¯æ¬¡æ•°å°‘
        - æ¯æ¬¡è·å–ä¸€å¤©çš„å…¨å¸‚åœºæ•°æ®ï¼Œæ•ˆç‡æ›´é«˜
        - æ›´ç¨³å®šï¼Œä¸å®¹æ˜“è§¦å‘APIé™åˆ¶
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            delay: æ¯æ¬¡è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
            exchange: äº¤æ˜“æ‰€
            
        Returns:
            pd.DataFrame: å…¨å¸‚åœºå†å²æ•°æ®
        """
        import time
        
        # è·å–äº¤æ˜“æ—¥å†
        trade_cal = self.get_trade_calendar(start_date, end_date, exchange)
        if trade_cal is None or trade_cal.empty:
            logger.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œé€€å‡ºæ•°æ®è·å–")
            return pd.DataFrame()
        
        trading_days = trade_cal['cal_date'].values
        total_days = len(trading_days)
        logger.info(f"å¼€å§‹é€šè¿‡äº¤æ˜“æ—¥å¾ªç¯è·å–æ•°æ®ï¼Œå…± {total_days} ä¸ªäº¤æ˜“æ—¥")
        
        all_data = []
        successful_days = 0
        
        for i, trade_date in enumerate(trading_days, 1):
            try:
                logger.info(f"æ­£åœ¨è·å– {trade_date} çš„å…¨å¸‚åœºæ•°æ® ({i}/{total_days})")
                
                # ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–æ•°æ®
                df = self.get_daily_with_retry(trade_date=trade_date)
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    successful_days += 1
                    logger.info(f"æˆåŠŸè·å– {trade_date} çš„ {len(df)} åªè‚¡ç¥¨æ•°æ®")
                else:
                    logger.warning(f"æœªè·å–åˆ° {trade_date} çš„æ•°æ®")
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_days:
                    success_rate = successful_days / i * 100
                    logger.info(f"è¿›åº¦: {i}/{total_days} ({i/total_days*100:.1f}%), "
                              f"æˆåŠŸè·å–: {successful_days}å¤© ({success_rate:.1f}%)")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"è·å– {trade_date} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•äº¤æ˜“æ—¥æ•°æ®")
            return pd.DataFrame()

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        total_records = len(combined_df)
        unique_stocks = combined_df['ts_code'].nunique() if 'ts_code' in combined_df.columns else 0
        
        logger.info(f"ğŸ‰ å…¨å¸‚åœºæ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»è®°å½•æ•°: {total_records:,} æ¡")
        logger.info(f"   ğŸ“ˆ æ¶‰åŠè‚¡ç¥¨: {unique_stocks} åª") 
        logger.info(f"   ğŸ“… äº¤æ˜“æ—¥æ•°: {successful_days}/{total_days}")
        logger.info(f"   âœ… æˆåŠŸç‡: {successful_days/total_days*100:.1f}%")
        
        return combined_df
    
    def get_all_market_data_by_dates_with_batch_insert(self, start_date: str, end_date: str, 
                                                      delay: float = 0.5, exchange: str = 'SSE',
                                                      db_instance=None, batch_days: int = 10) -> dict:
        """
        é€šè¿‡äº¤æ˜“æ—¥å¾ªç¯è·å–å…¨å¸‚åœºå†å²æ•°æ®ï¼Œå¹¶åˆ†æ‰¹æ’å…¥æ•°æ®åº“ï¼ˆæ¨èç”¨äºå¤§æ‰¹é‡æ•°æ®ï¼‰
        
        ä¼˜åŠ¿ï¼š
        - æŒ‰äº¤æ˜“æ—¥æ‰¹é‡æ’å…¥ï¼Œé¿å…å†…å­˜æº¢å‡º
        - å®æ—¶æ˜¾ç¤ºæ’å…¥è¿›åº¦
        - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆé¿å…é‡å¤æ’å…¥ï¼‰
        - æ›´å¥½çš„æ€§èƒ½å’Œç¨³å®šæ€§
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            delay: æ¯æ¬¡è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
            exchange: äº¤æ˜“æ‰€
            db_instance: æ•°æ®åº“å®ä¾‹
            batch_days: æ¯æ‰¹å¤„ç†çš„äº¤æ˜“æ—¥æ•°é‡
            
        Returns:
            dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        import time
        
        if db_instance is None:
            logger.error("éœ€è¦æä¾›æ•°æ®åº“å®ä¾‹è¿›è¡Œåˆ†æ‰¹æ’å…¥")
            return {}
        
        # è·å–äº¤æ˜“æ—¥å†
        trade_cal = self.get_trade_calendar(start_date, end_date, exchange)
        if trade_cal is None or trade_cal.empty:
            logger.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œé€€å‡ºæ•°æ®è·å–")
            return {}
        
        trading_days = trade_cal['cal_date'].values
        total_days = len(trading_days)
        logger.info(f"ğŸš€ å¼€å§‹å…¨å¸‚åœºæ•°æ®è·å–å’Œåˆ†æ‰¹æ’å…¥ï¼Œå…± {total_days} ä¸ªäº¤æ˜“æ—¥")
        logger.info(f"ğŸ“¦ åˆ†æ‰¹è®¾ç½®: æ¯ {batch_days} ä¸ªäº¤æ˜“æ—¥æ’å…¥ä¸€æ¬¡æ•°æ®åº“")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_trading_days': total_days,
            'successful_days': 0,
            'total_records': 0,
            'total_batches': 0,
            'failed_days': [],
            'batch_insert_success': 0,
            'batch_insert_failed': 0
        }
        
        current_batch_data = []
        batch_trading_days = []
        
        for i, trade_date in enumerate(trading_days, 1):
            try:
                logger.info(f"ğŸ“… æ­£åœ¨è·å– {trade_date} çš„å…¨å¸‚åœºæ•°æ® ({i}/{total_days})")
                
                # ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–æ•°æ®
                df = self.get_daily_with_retry(trade_date=trade_date)
                
                if df is not None and not df.empty:
                    current_batch_data.append(df)
                    batch_trading_days.append(trade_date)
                    stats['successful_days'] += 1
                    logger.info(f"âœ… æˆåŠŸè·å– {trade_date} çš„ {len(df)} åªè‚¡ç¥¨æ•°æ®")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {trade_date} çš„æ•°æ®")
                    stats['failed_days'].append(trade_date)
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(delay)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’å…¥æ•°æ®åº“
                should_insert = (
                    len(current_batch_data) >= batch_days or  # è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                    i == total_days or  # æ˜¯æœ€åä¸€ä¸ªäº¤æ˜“æ—¥
                    len(current_batch_data) >= 20  # æ•°æ®é‡è¾ƒå¤§æ—¶æå‰æ’å…¥
                )
                
                if should_insert and current_batch_data:
                    # åˆå¹¶å½“å‰æ‰¹æ¬¡æ•°æ®
                    batch_df = pd.concat(current_batch_data, ignore_index=True)
                    batch_records = len(batch_df)
                    
                    logger.info(f"ğŸ’¾ å¼€å§‹æ’å…¥ç¬¬ {stats['total_batches'] + 1} æ‰¹æ•°æ®...")
                    logger.info(f"   ğŸ“Š æœ¬æ‰¹æ•°æ®: {batch_records:,} æ¡è®°å½•")
                    logger.info(f"   ğŸ“… äº¤æ˜“æ—¥: {batch_trading_days[0]} åˆ° {batch_trading_days[-1]}")
                    
                    # æ’å…¥æ•°æ®åº“
                    insert_success = db_instance.insert_daily_data(batch_df)
                    
                    if insert_success:
                        stats['total_batches'] += 1
                        stats['total_records'] += batch_records
                        stats['batch_insert_success'] += 1
                        logger.info(f"âœ… ç¬¬ {stats['total_batches']} æ‰¹æ•°æ®æ’å…¥æˆåŠŸï¼")
                        logger.info(f"   ğŸ“ˆ ç´¯è®¡æ’å…¥: {stats['total_records']:,} æ¡è®°å½•")
                    else:
                        stats['batch_insert_failed'] += 1
                        logger.error(f"âŒ ç¬¬ {stats['total_batches'] + 1} æ‰¹æ•°æ®æ’å…¥å¤±è´¥")
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
                    current_batch_data = []
                    batch_trading_days = []
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_days:
                    success_rate = stats['successful_days'] / i * 100
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_days} ({i/total_days*100:.1f}%), "
                              f"æˆåŠŸè·å–: {stats['successful_days']}å¤© ({success_rate:.1f}%)")
                    logger.info(f"   ğŸ’¾ å·²æ’å…¥: {stats['total_records']:,} æ¡è®°å½•")
                
            except Exception as e:
                logger.error(f"âŒ è·å– {trade_date} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                stats['failed_days'].append(trade_date)
                continue
        
        # æœ€ç»ˆç»Ÿè®¡
        logger.info(f"ğŸ‰ å…¨å¸‚åœºæ•°æ®è·å–å’Œæ’å…¥å®Œæˆï¼")
        logger.info(f"   ğŸ“… æ€»äº¤æ˜“æ—¥: {stats['total_trading_days']} å¤©")
        logger.info(f"   âœ… æˆåŠŸè·å–: {stats['successful_days']} å¤©")
        logger.info(f"   ğŸ“Š æ€»æ’å…¥è®°å½•: {stats['total_records']:,} æ¡")
        logger.info(f"   ğŸ“¦ æ’å…¥æ‰¹æ¬¡: {stats['total_batches']} æ¬¡")
        logger.info(f"   ğŸ’¾ æ’å…¥æˆåŠŸç‡: {stats['batch_insert_success']}/{stats['total_batches']}")
        
        if stats['failed_days']:
            logger.warning(f"   âš ï¸ å¤±è´¥çš„äº¤æ˜“æ—¥: {len(stats['failed_days'])} å¤©")
            logger.debug(f"   å¤±è´¥æ—¥æœŸ: {stats['failed_days']}")
        
        return stats

    def get_all_index_daily_by_dates_with_batch_insert(
        self,
        start_date: str,
        end_date: str,
        delay: float = 0.5,
        exchange: str = "SSE",
        db_instance=None,
        batch_days: int = 10,
    ) -> dict:
        """
        è·å–â€œå…¨éƒ¨æŒ‡æ•°â€çš„æ—¥çº¿è¡Œæƒ…å¹¶åˆ†æ‰¹æ’å…¥ index_daily è¡¨ã€‚

        å—é™äº Tushare index_daily æ¥å£å½“å‰ç¯å¢ƒå¿…é¡»æä¾› ts_code å‚æ•°ï¼Œ
        è¿™é‡Œçš„å®ç°æ˜¯ï¼š
        1. å…ˆä» index_basic è¡¨æˆ–æ¥å£è·å–æ‰€æœ‰æŒ‡æ•°ä»£ç åˆ—è¡¨ï¼›
        2. å¯¹æ¯ä¸ª ts_code è°ƒç”¨ index_daily(ts_code, start_date, end_date)ï¼›
        3. ç´¯ç§¯åˆ°ä¸€å®šæ•°é‡åæ‰¹é‡å†™å…¥ index_dailyã€‚
        """
        import time

        if db_instance is None:
            logger.error("éœ€è¦æä¾›æ•°æ®åº“å®ä¾‹è¿›è¡ŒæŒ‡æ•°æ—¥çº¿åˆ†æ‰¹æ’å…¥")
            return {}

        # 1. è·å–æŒ‡æ•°ä»£ç åˆ—è¡¨ï¼ˆä¼˜å…ˆä»æ•°æ®åº“ index_basic è¡¨ï¼‰
        index_codes = []
        try:
            with db_instance.connection.cursor() as cursor:
                cursor.execute("SELECT ts_code FROM index_basic")
                rows = cursor.fetchall()
                index_codes = [r[0] for r in rows] if rows else []
        except Exception as e:
            logger.warning(f"ä» index_basic è¯»å–æŒ‡æ•°ä»£ç å¤±è´¥: {e}")

        if not index_codes:
            logger.info("index_basic ä¸­æš‚æ— æ•°æ®ï¼Œå°è¯•ä»APIè·å–æ‰€æœ‰æŒ‡æ•°åŸºæœ¬ä¿¡æ¯...")
            all_basic = self.get_all_index_basic_data()
            if all_basic is None or all_basic.empty:
                logger.error("æ— æ³•è·å–æŒ‡æ•°åŸºæœ¬ä¿¡æ¯ï¼Œé€€å‡ºæŒ‡æ•°æ—¥çº¿æ•°æ®è·å–")
                return {}
            # å°è¯•è½åº“ï¼Œä»¥ä¾¿åç»­å¤ç”¨
            try:
                db_instance.insert_index_basic(all_basic)
            except Exception as e:
                logger.warning(f"æ’å…¥æŒ‡æ•°åŸºæœ¬ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥ï¼ˆä¸å½±å“åç»­è¡Œæƒ…åˆå§‹åŒ–ï¼‰: {e}")
            index_codes = all_basic["ts_code"].tolist()

        total_indexes = len(index_codes)
        logger.info(f"ğŸš€ å¼€å§‹æŒ‡æ•°æ—¥çº¿æ•°æ®è·å–å’Œåˆ†æ‰¹æ’å…¥ï¼Œå…± {total_indexes} ä¸ªæŒ‡æ•°")

        stats = {
            "total_indexes": total_indexes,
            "successful_indexes": 0,
            "failed_indexes": 0,
            "total_records": 0,
            "total_batches": 0,
            "batch_insert_success": 0,
            "batch_insert_failed": 0,
            "failed_index_codes": [],
        }

        current_batch_data = []

        for i, ts_code in enumerate(index_codes, 1):
            try:
                logger.info(
                    f"ğŸ“ˆ æ­£åœ¨è·å–æŒ‡æ•° {ts_code} çš„æ—¥çº¿è¡Œæƒ… ({i}/{total_indexes}) "
                    f"[{start_date} ~ {end_date}]"
                )

                df = self.get_index_daily(
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date,
                )

                if df is not None and not df.empty:
                    current_batch_data.append(df)
                    stats["successful_indexes"] += 1
                    logger.info(
                        f"âœ… æˆåŠŸè·å– {ts_code} çš„ {len(df)} æ¡æŒ‡æ•°æ—¥çº¿æ•°æ®"
                    )
                else:
                    stats["failed_indexes"] += 1
                    stats["failed_index_codes"].append(ts_code)
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„æŒ‡æ•°æ—¥çº¿æ•°æ®")

                time.sleep(delay)

                should_insert = (
                    len(current_batch_data) >= batch_days or i == total_indexes
                )

                if should_insert and current_batch_data:
                    batch_df = pd.concat(current_batch_data, ignore_index=True)
                    batch_records = len(batch_df)

                    logger.info(
                        f"ğŸ’¾ å¼€å§‹æ’å…¥ç¬¬ {stats['total_batches'] + 1} æ‰¹æŒ‡æ•°æ—¥çº¿æ•°æ®..."
                    )
                    logger.info(f"   ğŸ“Š æœ¬æ‰¹æ•°æ®: {batch_records:,} æ¡è®°å½•")

                    insert_success = db_instance.insert_index_daily(batch_df)

                    if insert_success:
                        stats["total_batches"] += 1
                        stats["total_records"] += batch_records
                        stats["batch_insert_success"] += 1
                        logger.info(
                            f"âœ… ç¬¬ {stats['total_batches']} æ‰¹æŒ‡æ•°æ—¥çº¿æ•°æ®æ’å…¥æˆåŠŸï¼Œ"
                            f"ç´¯è®¡ {stats['total_records']:,} æ¡"
                        )
                    else:
                        stats["batch_insert_failed"] += 1
                        logger.error(
                            f"âŒ ç¬¬ {stats['total_batches'] + 1} æ‰¹æŒ‡æ•°æ—¥çº¿æ•°æ®æ’å…¥å¤±è´¥"
                        )

                    current_batch_data = []

                if i % 50 == 0 or i == total_indexes:
                    success_rate = (
                        stats["successful_indexes"] / i * 100 if i > 0 else 0
                    )
                    logger.info(
                        f"ğŸ“Š è¿›åº¦: {i}/{total_indexes} ({i/total_indexes*100:.1f}%), "
                        f"æˆåŠŸæŒ‡æ•°: {stats['successful_indexes']}, "
                        f"å¤±è´¥æŒ‡æ•°: {stats['failed_indexes']} "
                        f"({success_rate:.1f}% æˆåŠŸç‡)"
                    )
                    logger.info(
                        f"   ğŸ’¾ å·²æ’å…¥æŒ‡æ•°æ—¥çº¿: {stats['total_records']:,} æ¡è®°å½•"
                    )

            except Exception as e:
                stats["failed_indexes"] += 1
                stats["failed_index_codes"].append(ts_code)
                logger.error(f"âŒ è·å–æŒ‡æ•° {ts_code} æ—¥çº¿æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue

        logger.info("ğŸ‰ æŒ‡æ•°æ—¥çº¿æ•°æ®è·å–å’Œæ’å…¥å®Œæˆï¼")
        logger.info(f"   ğŸ“ˆ æ€»æŒ‡æ•°æ•°: {stats['total_indexes']} ä¸ª")
        logger.info(f"   âœ… æˆåŠŸæŒ‡æ•°: {stats['successful_indexes']} ä¸ª")
        logger.info(f"   ğŸ“Š æ€»æ’å…¥è®°å½•: {stats['total_records']:,} æ¡")
        logger.info(f"   ğŸ“¦ æ’å…¥æ‰¹æ¬¡: {stats['total_batches']} æ¬¡")
        logger.info(
            f"   ğŸ’¾ æ’å…¥æˆåŠŸç‡: {stats['batch_insert_success']}/{stats['total_batches']}"
        )

        if stats["failed_index_codes"]:
            logger.warning(
                f"   âš ï¸ æ‹‰å–å¤±è´¥çš„æŒ‡æ•°: {len(stats['failed_index_codes'])} ä¸ª "
                f"(ç¤ºä¾‹: {stats['failed_index_codes'][:5]})"
            )

        return stats

    def get_all_etf_daily_by_dates_with_batch_insert(self, start_date: str, end_date: str,
                                                     delay: float = 0.5, exchange: str = 'SSE',
                                                     db_instance=None, batch_days: int = 10) -> dict:
        """
        æŒ‰äº¤æ˜“æ—¥å¾ªç¯è·å– ETF æ—¥çº¿è¡Œæƒ…ï¼ˆfund_dailyï¼‰å¹¶åˆ†æ‰¹æ’å…¥ etf_daily è¡¨

        Args:
            start_date: å¼€å§‹æ—¥æœŸ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸ YYYYMMDD
            delay: æ¯å¤©è¯·æ±‚çš„å»¶è¿Ÿ
            exchange: ç”¨äºå–äº¤æ˜“æ—¥å†çš„äº¤æ˜“æ‰€ (SSE/SZSE)
            db_instance: æ•°æ®åº“å®ä¾‹ (éœ€è¦æ”¯æŒ insert_etf_daily)
            batch_days: æ¯æ‰¹æ’å…¥çš„äº¤æ˜“æ—¥æ•°é‡

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        import time

        if db_instance is None:
            logger.error("éœ€è¦æä¾›æ•°æ®åº“å®ä¾‹è¿›è¡ŒETFæ—¥çº¿åˆ†æ‰¹æ’å…¥")
            return {}

        # è·å–äº¤æ˜“æ—¥å†
        trade_cal = self.get_trade_calendar(start_date, end_date, exchange)
        if trade_cal is None or trade_cal.empty:
            logger.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œé€€å‡ºETFæ—¥çº¿æ•°æ®è·å–")
            return {}

        trading_days = trade_cal["cal_date"].values
        total_days = len(trading_days)
        logger.info(f"ğŸš€ å¼€å§‹ETFæ—¥çº¿æ•°æ®è·å–å’Œåˆ†æ‰¹æ’å…¥ï¼Œå…± {total_days} ä¸ªäº¤æ˜“æ—¥")

        stats = {
            "total_trading_days": total_days,
            "successful_days": 0,
            "total_records": 0,
            "total_batches": 0,
            "failed_days": [],
            "batch_insert_success": 0,
            "batch_insert_failed": 0,
        }

        current_batch_data = []
        batch_trading_days = []

        for i, trade_date in enumerate(trading_days, 1):
            try:
                logger.info(f"ğŸ“… æ­£åœ¨è·å– {trade_date} çš„ETFæ—¥çº¿æ•°æ® ({i}/{total_days})")

                df = self.get_etf_daily_with_retry(trade_date=trade_date)

                if df is not None and not df.empty:
                    current_batch_data.append(df)
                    batch_trading_days.append(trade_date)
                    stats["successful_days"] += 1
                    logger.info(f"âœ… æˆåŠŸè·å– {trade_date} çš„ {len(df)} æ¡ETFæ—¥çº¿æ•°æ®")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {trade_date} çš„ETFæ—¥çº¿æ•°æ®")
                    stats["failed_days"].append(trade_date)

                time.sleep(delay)

                should_insert = (
                    len(current_batch_data) >= batch_days
                    or i == total_days
                    or len(current_batch_data) >= 20
                )

                if should_insert and current_batch_data:
                    batch_df = pd.concat(current_batch_data, ignore_index=True)
                    batch_records = len(batch_df)

                    logger.info(f"ğŸ’¾ å¼€å§‹æ’å…¥ç¬¬ {stats['total_batches'] + 1} æ‰¹ETFæ—¥çº¿æ•°æ®...")
                    logger.info(
                        f"   ğŸ“Š æœ¬æ‰¹æ•°æ®: {batch_records:,} æ¡, äº¤æ˜“æ—¥: {batch_trading_days[0]} ~ {batch_trading_days[-1]}"
                    )

                    insert_success = db_instance.insert_etf_daily(batch_df)

                    if insert_success:
                        stats["total_batches"] += 1
                        stats["total_records"] += batch_records
                        stats["batch_insert_success"] += 1
                        logger.info(
                            f"âœ… ç¬¬ {stats['total_batches']} æ‰¹ETFæ—¥çº¿æ•°æ®æ’å…¥æˆåŠŸï¼Œç´¯è®¡ {stats['total_records']:,} æ¡"
                        )
                    else:
                        stats["batch_insert_failed"] += 1
                        logger.error(f"âŒ ç¬¬ {stats['total_batches'] + 1} æ‰¹ETFæ—¥çº¿æ•°æ®æ’å…¥å¤±è´¥")

                    current_batch_data = []
                    batch_trading_days = []

                if i % 10 == 0 or i == total_days:
                    success_rate = stats["successful_days"] / i * 100
                    logger.info(
                        f"ğŸ“Š è¿›åº¦: {i}/{total_days} ({i/total_days*100:.1f}%), "
                        f"æˆåŠŸè·å–: {stats['successful_days']}å¤© ({success_rate:.1f}%)"
                    )
                    logger.info(f"   ğŸ’¾ å·²æ’å…¥ETFæ—¥çº¿: {stats['total_records']:,} æ¡è®°å½•")

            except Exception as e:
                logger.error(f"âŒ è·å– {trade_date} ETFæ—¥çº¿æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                stats["failed_days"].append(trade_date)
                continue

        logger.info("ğŸ‰ ETFæ—¥çº¿æ•°æ®è·å–å’Œæ’å…¥å®Œæˆï¼")
        logger.info(f"   ğŸ“… æ€»äº¤æ˜“æ—¥: {stats['total_trading_days']} å¤©")
        logger.info(f"   âœ… æˆåŠŸè·å–: {stats['successful_days']} å¤©")
        logger.info(f"   ğŸ“Š æ€»æ’å…¥è®°å½•: {stats['total_records']:,} æ¡")
        logger.info(f"   ğŸ“¦ æ’å…¥æ‰¹æ¬¡: {stats['total_batches']} æ¬¡")
        logger.info(
            f"   ğŸ’¾ æ’å…¥æˆåŠŸç‡: {stats['batch_insert_success']}/{stats['total_batches']}"
        )

        if stats["failed_days"]:
            logger.warning(f"   âš ï¸ å¤±è´¥çš„äº¤æ˜“æ—¥: {len(stats['failed_days'])} å¤©")

        return stats
    
    def estimate_market_data_time(self, start_date: str, end_date: str, delay: float = 0.5) -> str:
        """
        é¢„ä¼°å…¨å¸‚åœºæ•°æ®è·å–æ—¶é—´
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ  
            delay: å»¶è¿Ÿæ—¶é—´
            
        Returns:
            str: é¢„ä¼°æ—¶é—´æè¿°
        """
        # ç²—ç•¥ä¼°ç®—äº¤æ˜“æ—¥æ•°é‡ï¼ˆæ¯å¹´çº¦220ä¸ªäº¤æ˜“æ—¥ï¼‰
        from datetime import datetime
        try:
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            days_diff = (end_dt - start_dt).days
            estimated_trading_days = int(days_diff * 220 / 365)  # ç²—ç•¥ä¼°ç®—
        except:
            estimated_trading_days = 220  # é»˜è®¤ä¸€å¹´
        
        total_seconds = estimated_trading_days * delay
        
        if total_seconds < 60:
            return f"{total_seconds:.0f}ç§’"
        elif total_seconds < 3600:
            return f"{total_seconds/60:.1f}åˆ†é’Ÿ"
        else:
            return f"{total_seconds/3600:.1f}å°æ—¶"
    
    def get_ths_index(self, ts_code: str = None, exchange: str = None, 
                     index_type: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®
        
        æ ¹æ®Tushareæ–‡æ¡£ï¼Œéœ€è¦5000ç§¯åˆ†æƒé™ï¼Œå•æ¬¡æœ€å¤§è¿”å›5000è¡Œæ•°æ®
        
        Args:
            ts_code: æŒ‡æ•°ä»£ç 
            exchange: å¸‚åœºç±»å‹ A-aè‚¡ HK-æ¸¯è‚¡ US-ç¾è‚¡
            index_type: æŒ‡æ•°ç±»å‹ N-æ¦‚å¿µæŒ‡æ•° I-è¡Œä¸šæŒ‡æ•° R-åœ°åŸŸæŒ‡æ•° S-åŒèŠ±é¡ºç‰¹è‰²æŒ‡æ•° 
                       ST-åŒèŠ±é¡ºé£æ ¼æŒ‡æ•° TH-åŒèŠ±é¡ºä¸»é¢˜æŒ‡æ•° BB-åŒèŠ±é¡ºå®½åŸºæŒ‡æ•°
            
        Returns:
            pd.DataFrame: åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®
        """
        try:
            logger.info("æ­£åœ¨è·å–åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®...")
            
            # æ„å»ºå‚æ•°å­—å…¸
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if exchange:
                params['exchange'] = exchange
            if index_type:
                params['type'] = index_type
            
            # è°ƒç”¨Tushare API
            df = self.pro.ths_index(**params)
            
            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            if 'list_date' in df.columns:
                # å°†list_dateè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                df['list_date'] = pd.to_datetime(df['list_date'], format='%Y%m%d', errors='coerce')
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if 'type' in df.columns:
                type_counts = df['type'].value_counts()
                logger.info("æŒ‡æ•°ç±»å‹åˆ†å¸ƒï¼š")
                for idx_type, count in type_counts.items():
                    type_name = self._get_index_type_name(idx_type)
                    logger.info(f"  {type_name}({idx_type}): {count}ä¸ª")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæŒ‡æ•°ä¿¡æ¯
            logger.info("éƒ¨åˆ†æŒ‡æ•°ç¤ºä¾‹ï¼š")
            for i, (_, row) in enumerate(df.head(3).iterrows()):
                type_name = self._get_index_type_name(row.get('type', ''))
                logger.info(f"  {row.get('name', 'N/A')}({row.get('ts_code', 'N/A')}) - {type_name} - æˆåˆ†è‚¡:{row.get('count', 'N/A')}ä¸ª")
                
            return df
            
        except Exception as e:
            logger.error(f"è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é—®é¢˜
            if "æƒé™" in str(e) or "ç§¯åˆ†" in str(e) or "permission" in str(e).lower():
                logger.error("å¯èƒ½æ˜¯æƒé™ä¸è¶³ï¼Œéœ€è¦5000ç§¯åˆ†æ‰èƒ½è°ƒç”¨ths_indexæ¥å£")
                logger.info("è¯·æ£€æŸ¥æ‚¨çš„Tushareè´¦æˆ·ç§¯åˆ†æˆ–å‡çº§è´¦æˆ·æƒé™")
            
            return None
    
    def _get_index_type_name(self, index_type: str) -> str:
        """
        è·å–æŒ‡æ•°ç±»å‹ä¸­æ–‡åç§°
        
        Args:
            index_type: æŒ‡æ•°ç±»å‹ä»£ç 
            
        Returns:
            str: ä¸­æ–‡åç§°
        """
        type_mapping = {
            'N': 'æ¦‚å¿µæŒ‡æ•°',
            'I': 'è¡Œä¸šæŒ‡æ•°', 
            'R': 'åœ°åŸŸæŒ‡æ•°',
            'S': 'åŒèŠ±é¡ºç‰¹è‰²æŒ‡æ•°',
            'ST': 'åŒèŠ±é¡ºé£æ ¼æŒ‡æ•°',
            'TH': 'åŒèŠ±é¡ºä¸»é¢˜æŒ‡æ•°',
            'BB': 'åŒèŠ±é¡ºå®½åŸºæŒ‡æ•°'
        }
        return type_mapping.get(index_type, 'æœªçŸ¥ç±»å‹')
    
    def get_all_ths_index_data(self) -> Optional[pd.DataFrame]:
        """
        è·å–æ‰€æœ‰åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®ï¼ˆåˆ†ç±»å‹è·å–ï¼‰
        
        ç”±äºAPIå•æ¬¡è°ƒç”¨é™åˆ¶5000æ¡ï¼Œè¿™é‡Œåˆ†ç±»å‹è·å–ä»¥ç¡®ä¿è·å–å®Œæ•´æ•°æ®
        
        Returns:
            pd.DataFrame: æ‰€æœ‰æŒ‡æ•°æ•°æ®
        """
        logger.info("ğŸš€ å¼€å§‹è·å–æ‰€æœ‰åŒèŠ±é¡ºæ¦‚å¿µå’Œè¡Œä¸šæŒ‡æ•°æ•°æ®...")
        
        # å®šä¹‰è¦è·å–çš„æŒ‡æ•°ç±»å‹
        index_types = ['N', 'I', 'R', 'S', 'ST', 'TH', 'BB']
        all_data = []
        
        for index_type in index_types:
            try:
                type_name = self._get_index_type_name(index_type)
                logger.info(f"æ­£åœ¨è·å–{type_name}({index_type})...")
                
                df = self.get_ths_index(index_type=index_type)
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    logger.info(f"âœ… æˆåŠŸè·å–{type_name} {len(df)} ä¸ªæŒ‡æ•°")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°{type_name}æ•°æ®")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                import time
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"âŒ è·å–{type_name}æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®")
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        
        logger.info(f"ğŸ‰ åŒèŠ±é¡ºæŒ‡æ•°æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»æŒ‡æ•°æ•°é‡: {len(combined_df)} ä¸ª")
        
        # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        if 'type' in combined_df.columns:
            type_summary = combined_df['type'].value_counts()
            logger.info("ğŸ“ˆ æŒ‡æ•°ç±»å‹æ±‡æ€»ï¼š")
            for idx_type, count in type_summary.items():
                type_name = self._get_index_type_name(idx_type)
                logger.info(f"   {type_name}: {count} ä¸ª")
        
        return combined_df
    
    def get_ths_member(self, ts_code: str = None, con_code: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
        
        æ ¹æ®Tushareæ–‡æ¡£ï¼Œéœ€è¦5000ç§¯åˆ†æƒé™ï¼Œæ¯åˆ†é’Ÿå¯è°ƒå–200æ¬¡
        
        Args:
            ts_code: æ¿å—æŒ‡æ•°ä»£ç 
            con_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            pd.DataFrame: æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®...")
            
            # æ„å»ºå‚æ•°å­—å…¸
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if con_code:
                params['con_code'] = con_code
            
            # è°ƒç”¨Tushare API
            df = self.pro.ths_member(**params)
            
            if df is None or df.empty:
                logger.warning(f"æœªè·å–åˆ°æŒ‡æ•° {ts_code} çš„æˆåˆ†è‚¡æ•°æ®")
                return None
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡æˆåˆ†è‚¡æ•°æ®")
            
            # æ˜¾ç¤ºæˆåˆ†è‚¡ä¿¡æ¯
            if len(df) > 0:
                logger.info(f"æˆåˆ†è‚¡ç¤ºä¾‹ï¼š")
                for i, (_, row) in enumerate(df.head(3).iterrows()):
                    logger.info(f"  {row.get('con_name', 'N/A')}({row.get('con_code', 'N/A')})")
                    
            return df
            
        except Exception as e:
            logger.error(f"è·å–åŒèŠ±é¡ºæ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é—®é¢˜
            if "æƒé™" in str(e) or "ç§¯åˆ†" in str(e) or "permission" in str(e).lower():
                logger.error("å¯èƒ½æ˜¯æƒé™ä¸è¶³ï¼Œéœ€è¦5000ç§¯åˆ†æ‰èƒ½è°ƒç”¨ths_memberæ¥å£")
                logger.info("è¯·æ£€æŸ¥æ‚¨çš„Tushareè´¦æˆ·ç§¯åˆ†æˆ–å‡çº§è´¦æˆ·æƒé™")
            
            return None
    
    def get_all_concept_members(self, concept_indexes: List[str] = None, 
                               batch_delay: float = 0.3) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰æ¦‚å¿µæŒ‡æ•°çš„æˆåˆ†è‚¡æ•°æ®
        
        Args:
            concept_indexes: æ¦‚å¿µæŒ‡æ•°ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä»æ•°æ®åº“ä¸­è·å–
            batch_delay: æ¯æ¬¡APIè°ƒç”¨çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è§¦å‘é¢‘ç‡é™åˆ¶
            
        Returns:
            pd.DataFrame: æ‰€æœ‰æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
        """
        import time
        
        logger.info("ğŸš€ å¼€å§‹è·å–æ‰€æœ‰æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®...")
        
        # å¦‚æœæ²¡æœ‰æä¾›æŒ‡æ•°åˆ—è¡¨ï¼Œä»æ•°æ®åº“è·å–æ¦‚å¿µæŒ‡æ•°
        if concept_indexes is None:
            try:
                from database import StockDatabase
                with StockDatabase() as db:
                    # åªè·å–æ¦‚å¿µæŒ‡æ•°(N)
                    concept_df = db.query_ths_index(index_type='N')
                    if concept_df is not None and not concept_df.empty:
                        concept_indexes = concept_df['ts_code'].tolist()
                        logger.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(concept_indexes)} ä¸ªæ¦‚å¿µæŒ‡æ•°")
                    else:
                        logger.error("æ•°æ®åº“ä¸­æ²¡æœ‰æ¦‚å¿µæŒ‡æ•°æ•°æ®")
                        return pd.DataFrame()
            except Exception as e:
                logger.error(f"ä»æ•°æ®åº“è·å–æ¦‚å¿µæŒ‡æ•°å¤±è´¥: {e}")
                return pd.DataFrame()
        
        if not concept_indexes:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¦‚å¿µæŒ‡æ•°åˆ—è¡¨")
            return pd.DataFrame()
        
        all_members_data = []
        total_indexes = len(concept_indexes)
        successful_count = 0
        failed_count = 0
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {total_indexes} ä¸ªæ¦‚å¿µæŒ‡æ•°çš„æˆåˆ†è‚¡æ•°æ®")
        
        for i, ts_code in enumerate(concept_indexes, 1):
            try:
                logger.info(f"æ­£åœ¨è·å–æŒ‡æ•° {ts_code} çš„æˆåˆ†è‚¡ ({i}/{total_indexes})")
                
                # è·å–å•ä¸ªæŒ‡æ•°çš„æˆåˆ†è‚¡
                df = self.get_ths_member(ts_code=ts_code)
                
                if df is not None and not df.empty:
                    all_members_data.append(df)
                    successful_count += 1
                    logger.info(f"âœ… æˆåŠŸè·å– {ts_code} çš„ {len(df)} åªæˆåˆ†è‚¡")
                else:
                    failed_count += 1
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„æˆåˆ†è‚¡æ•°æ®")
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_indexes:
                    success_rate = successful_count / i * 100
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_indexes} ({i/total_indexes*100:.1f}%), "
                              f"æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count} ({success_rate:.1f}%)")
                
                # APIè°ƒç”¨å»¶è¿Ÿï¼Œé˜²æ­¢é¢‘ç‡é™åˆ¶
                time.sleep(batch_delay)
                
            except Exception as e:
                failed_count += 1
                logger.error(f"âŒ è·å– {ts_code} æˆåˆ†è‚¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_members_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_members_data, ignore_index=True)
        
        logger.info(f"ğŸ‰ æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»æˆåˆ†è‚¡è®°å½•: {len(combined_df):,} æ¡")
        logger.info(f"   ğŸ“ˆ æ¶‰åŠæŒ‡æ•°: {successful_count} ä¸ª")
        logger.info(f"   ğŸ“ˆ ä¸é‡å¤è‚¡ç¥¨: {combined_df['con_code'].nunique() if 'con_code' in combined_df.columns else 0} åª")
        logger.info(f"   âœ… æˆåŠŸç‡: {successful_count}/{total_indexes} ({successful_count/total_indexes*100:.1f}%)")
        
        return combined_df
    
    def get_concept_members_batch_with_db_insert(self, db_instance=None, 
                                               concept_indexes: List[str] = None,
                                               batch_delay: float = 0.3,
                                               batch_size: int = 20) -> dict:
        """
        æ‰¹é‡è·å–æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡å¹¶åˆ†æ‰¹æ’å…¥æ•°æ®åº“
        
        Args:
            db_instance: æ•°æ®åº“å®ä¾‹
            concept_indexes: æ¦‚å¿µæŒ‡æ•°ä»£ç åˆ—è¡¨
            batch_delay: APIè°ƒç”¨å»¶è¿Ÿ
            batch_size: åˆ†æ‰¹æ’å…¥çš„æ•°é‡
            
        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        import time
        
        if db_instance is None:
            logger.error("éœ€è¦æä¾›æ•°æ®åº“å®ä¾‹è¿›è¡Œåˆ†æ‰¹æ’å…¥")
            return {}
        
        # å¦‚æœæ²¡æœ‰æä¾›æŒ‡æ•°åˆ—è¡¨ï¼Œä»æ•°æ®åº“è·å–æ¦‚å¿µæŒ‡æ•°
        if concept_indexes is None:
            concept_df = db_instance.query_ths_index(index_type='N')
            if concept_df is not None and not concept_df.empty:
                concept_indexes = concept_df['ts_code'].tolist()
                logger.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(concept_indexes)} ä¸ªæ¦‚å¿µæŒ‡æ•°")
            else:
                logger.error("æ•°æ®åº“ä¸­æ²¡æœ‰æ¦‚å¿µæŒ‡æ•°æ•°æ®")
                return {}
        
        if not concept_indexes:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¦‚å¿µæŒ‡æ•°åˆ—è¡¨")
            return {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_indexes': len(concept_indexes),
            'successful_indexes': 0,
            'failed_indexes': 0,
            'total_members': 0,
            'batch_count': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'failed_index_codes': []
        }
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è·å–å¹¶æ’å…¥ {stats['total_indexes']} ä¸ªæ¦‚å¿µæŒ‡æ•°çš„æˆåˆ†è‚¡æ•°æ®")
        logger.info(f"ğŸ“¦ åˆ†æ‰¹è®¾ç½®: æ¯ {batch_size} ä¸ªæŒ‡æ•°æ’å…¥ä¸€æ¬¡æ•°æ®åº“")
        
        current_batch_data = []
        
        for i, ts_code in enumerate(concept_indexes, 1):
            try:
                logger.info(f"ğŸ“Š æ­£åœ¨è·å–æŒ‡æ•° {ts_code} çš„æˆåˆ†è‚¡ ({i}/{stats['total_indexes']})")
                
                # è·å–å•ä¸ªæŒ‡æ•°çš„æˆåˆ†è‚¡
                df = self.get_ths_member(ts_code=ts_code)
                
                if df is not None and not df.empty:
                    current_batch_data.append(df)
                    stats['successful_indexes'] += 1
                    stats['total_members'] += len(df)
                    logger.info(f"âœ… æˆåŠŸè·å– {ts_code} çš„ {len(df)} åªæˆåˆ†è‚¡")
                else:
                    stats['failed_indexes'] += 1
                    stats['failed_index_codes'].append(ts_code)
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„æˆåˆ†è‚¡æ•°æ®")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(batch_delay)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’å…¥æ•°æ®åº“
                should_insert = (
                    len(current_batch_data) >= batch_size or  # è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                    i == stats['total_indexes']  # æ˜¯æœ€åä¸€ä¸ªæŒ‡æ•°
                )
                
                if should_insert and current_batch_data:
                    # åˆå¹¶å½“å‰æ‰¹æ¬¡æ•°æ®
                    batch_df = pd.concat(current_batch_data, ignore_index=True)
                    batch_records = len(batch_df)
                    
                    logger.info(f"ğŸ’¾ å¼€å§‹æ’å…¥ç¬¬ {stats['batch_count'] + 1} æ‰¹æ•°æ®...")
                    logger.info(f"   ğŸ“Š æœ¬æ‰¹æ•°æ®: {batch_records:,} æ¡æˆåˆ†è‚¡è®°å½•")
                    
                    # æ’å…¥æ•°æ®åº“
                    insert_success = db_instance.insert_ths_member(batch_df)
                    
                    if insert_success:
                        stats['batch_count'] += 1
                        stats['successful_batches'] += 1
                        logger.info(f"âœ… ç¬¬ {stats['batch_count']} æ‰¹æ•°æ®æ’å…¥æˆåŠŸï¼")
                    else:
                        stats['failed_batches'] += 1
                        logger.error(f"âŒ ç¬¬ {stats['batch_count'] + 1} æ‰¹æ•°æ®æ’å…¥å¤±è´¥")
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
                    current_batch_data = []
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == stats['total_indexes']:
                    success_rate = stats['successful_indexes'] / i * 100
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{stats['total_indexes']} ({i/stats['total_indexes']*100:.1f}%), "
                              f"æˆåŠŸ: {stats['successful_indexes']}, å¤±è´¥: {stats['failed_indexes']} ({success_rate:.1f}%)")
                    logger.info(f"   ğŸ’¾ å·²æ’å…¥: {stats['total_members']:,} æ¡æˆåˆ†è‚¡è®°å½•")
                
            except Exception as e:
                stats['failed_indexes'] += 1
                stats['failed_index_codes'].append(ts_code)
                logger.error(f"âŒ è·å– {ts_code} æˆåˆ†è‚¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        # æœ€ç»ˆç»Ÿè®¡
        logger.info(f"ğŸ‰ æ¦‚å¿µæŒ‡æ•°æˆåˆ†è‚¡æ•°æ®è·å–å’Œæ’å…¥å®Œæˆï¼")
        logger.info(f"   ğŸ“Š å¤„ç†æŒ‡æ•°: {stats['total_indexes']} ä¸ª")
        logger.info(f"   âœ… æˆåŠŸæŒ‡æ•°: {stats['successful_indexes']} ä¸ª")
        logger.info(f"   ğŸ“Š æ€»æˆåˆ†è‚¡è®°å½•: {stats['total_members']:,} æ¡")
        logger.info(f"   ğŸ“¦ æ’å…¥æ‰¹æ¬¡: {stats['batch_count']} æ¬¡")
        logger.info(f"   ğŸ’¾ æ’å…¥æˆåŠŸç‡: {stats['successful_batches']}/{stats['batch_count']}")
        
        if stats['failed_index_codes']:
            logger.warning(f"   âš ï¸ å¤±è´¥çš„æŒ‡æ•°: {len(stats['failed_index_codes'])} ä¸ª")
            logger.debug(f"   å¤±è´¥æŒ‡æ•°ä»£ç : {stats['failed_index_codes']}")
        
        return stats
    
    def get_index_basic(self, ts_code: str = None, name: str = None, market: str = None,
                       publisher: str = None, category: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°åŸºç¡€ä¿¡æ¯
        
        Args:
            ts_code: æŒ‡æ•°ä»£ç 
            name: æŒ‡æ•°ç®€ç§°
            market: äº¤æ˜“æ‰€æˆ–æœåŠ¡å•†(é»˜è®¤SSE)
            publisher: å‘å¸ƒå•†
            category: æŒ‡æ•°ç±»åˆ«
            
        Returns:
            pd.DataFrame: æŒ‡æ•°åŸºæœ¬ä¿¡æ¯
        """
        try:
            logger.info("æ­£åœ¨è·å–æŒ‡æ•°åŸºç¡€ä¿¡æ¯...")
            
            # æ„å»ºå‚æ•°å­—å…¸
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if name:
                params['name'] = name
            if market:
                params['market'] = market
            if publisher:
                params['publisher'] = publisher
            if category:
                params['category'] = category
            
            # è°ƒç”¨Tushare API
            df = self.pro.index_basic(**params)
            
            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°æŒ‡æ•°åŸºç¡€ä¿¡æ¯")
                return None
                
            # æ•°æ®é¢„å¤„ç†
            if 'base_date' in df.columns:
                df['base_date'] = pd.to_datetime(df['base_date'], format='%Y%m%d', errors='coerce')
            if 'list_date' in df.columns:
                df['list_date'] = pd.to_datetime(df['list_date'], format='%Y%m%d', errors='coerce')
            if 'exp_date' in df.columns:
                df['exp_date'] = pd.to_datetime(df['exp_date'], format='%Y%m%d', errors='coerce')
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡æŒ‡æ•°åŸºç¡€ä¿¡æ¯")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if 'market' in df.columns:
                market_counts = df['market'].value_counts()
                logger.info("å¸‚åœºåˆ†å¸ƒï¼š")
                for market, count in market_counts.items():
                    logger.info(f"  {market}: {count}ä¸ª")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæŒ‡æ•°ä¿¡æ¯
            logger.info("éƒ¨åˆ†æŒ‡æ•°ç¤ºä¾‹ï¼š")
            for i, (_, row) in enumerate(df.head(3).iterrows()):
                logger.info(f"  {row.get('name', 'N/A')}({row.get('ts_code', 'N/A')}) "
                          f"- {row.get('market', 'N/A')} - å‘å¸ƒæ–¹:{row.get('publisher', 'N/A')}")
            
            return df
            
        except Exception as e:
            logger.error(f"è·å–æŒ‡æ•°åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def get_index_daily(self, ts_code: str = None, trade_date: str = None, 
                       start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°æ—¥çº¿è¡Œæƒ…æ•°æ®
        
        Args:
            ts_code: æŒ‡æ•°ä»£ç 
            trade_date: äº¤æ˜“æ—¥æœŸ(YYYYMMDDæ ¼å¼)
            start_date: å¼€å§‹æ—¥æœŸ(YYYYMMDDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ(YYYYMMDDæ ¼å¼)
            
        Returns:
            pd.DataFrame: æŒ‡æ•°æ—¥çº¿è¡Œæƒ…æ•°æ®
        """
        try:
            if ts_code:
                logger.info(f"æ­£åœ¨è·å–æŒ‡æ•° {ts_code} çš„æ—¥çº¿è¡Œæƒ…æ•°æ®...")
            else:
                logger.info("æ­£åœ¨è·å–æŒ‡æ•°æ—¥çº¿è¡Œæƒ…æ•°æ®...")
            
            # æ„å»ºå‚æ•°å­—å…¸
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if trade_date:
                params['trade_date'] = trade_date
            if start_date:
                params['start_date'] = start_date
            if end_date:
                params['end_date'] = end_date
            
            # è°ƒç”¨Tushare API
            df = self.pro.index_daily(**params)
            
            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°æŒ‡æ•°æ—¥çº¿è¡Œæƒ…æ•°æ®")
                return None
                
            # æ•°æ®é¢„å¤„ç†
            if 'trade_date' in df.columns:
                df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡æŒ‡æ•°æ—¥çº¿è¡Œæƒ…æ•°æ®")
            
            return df
            
        except Exception as e:
            logger.error(f"è·å–æŒ‡æ•°æ—¥çº¿è¡Œæƒ…å¤±è´¥: {e}")
            return None

    def get_index_weekly(self, ts_code: str = None, trade_date: str = None,
                         start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®

        å¯¹åº” Tushare index_weekly æ¥å£ï¼Œæ–‡æ¡£å‚è€ƒ:
        https://tushare.pro/document/2?doc_id=171

        Args:
            ts_code: æŒ‡æ•°ä»£ç 
            trade_date: äº¤æ˜“æ—¥æœŸ(YYYYMMDDæ ¼å¼)
            start_date: å¼€å§‹æ—¥æœŸ(YYYYMMDDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ(YYYYMMDDæ ¼å¼)

        Returns:
            pd.DataFrame: æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®
        """
        try:
            if ts_code:
                logger.info(f"æ­£åœ¨è·å–æŒ‡æ•° {ts_code} çš„å‘¨çº¿è¡Œæƒ…æ•°æ®...")
            else:
                logger.info("æ­£åœ¨è·å–æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®...")

            params = {}
            if ts_code:
                params["ts_code"] = ts_code
            if trade_date:
                params["trade_date"] = trade_date
            if start_date:
                params["start_date"] = start_date
            if end_date:
                params["end_date"] = end_date

            df = self.pro.index_weekly(**params)

            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®")
                return None

            if "trade_date" in df.columns:
                df["trade_date"] = pd.to_datetime(df["trade_date"], format="%Y%m%d")

            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®")
            return df

        except Exception as e:
            logger.error(f"è·å–æŒ‡æ•°å‘¨çº¿è¡Œæƒ…å¤±è´¥: {e}")
            return None

    def get_index_weight(self, index_code: str = None, trade_date: str = None,
                         start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°æˆåˆ†å’Œæƒé‡æ•°æ®

        å¯¹åº”Tushare index_weightæ¥å£ï¼Œæ–‡æ¡£å‚è€ƒ:
        https://tushare.pro/document/2?doc_id=171 (æˆ–ç›¸å…³æŒ‡æ•°æˆåˆ†æƒé‡æ–‡æ¡£)

        Args:
            index_code: æŒ‡æ•°ä»£ç ï¼Œå¦‚ '000300.SH'
            trade_date: äº¤æ˜“æ—¥æœŸ (YYYYMMDD)
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)

        Returns:
            pd.DataFrame: æŒ‡æ•°æˆåˆ†å’Œæƒé‡æ•°æ®
        """
        try:
            logger.info("æ­£åœ¨è·å–æŒ‡æ•°æˆåˆ†å’Œæƒé‡æ•°æ®...")

            params = {}
            if index_code:
                params["index_code"] = index_code
            if trade_date:
                params["trade_date"] = trade_date
            if start_date:
                params["start_date"] = start_date
            if end_date:
                params["end_date"] = end_date

            df = self.pro.index_weight(**params)

            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°æŒ‡æ•°æˆåˆ†å’Œæƒé‡æ•°æ®")
                return None

            # trade_date å­—æ®µè½¬æ¢ä¸ºæ—¥æœŸ
            if "trade_date" in df.columns:
                df["trade_date"] = pd.to_datetime(df["trade_date"], format="%Y%m%d", errors="coerce")

            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡æŒ‡æ•°æˆåˆ†å’Œæƒé‡æ•°æ®")
            return df

        except Exception as e:
            logger.error(f"è·å–æŒ‡æ•°æˆåˆ†å’Œæƒé‡æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_all_index_basic_data(self) -> Optional[pd.DataFrame]:
        """
        è·å–æ‰€æœ‰æŒ‡æ•°åŸºç¡€ä¿¡æ¯æ•°æ®ï¼ˆåˆ†å¸‚åœºè·å–ï¼‰
        
        Returns:
            pd.DataFrame: æ‰€æœ‰æŒ‡æ•°åŸºç¡€ä¿¡æ¯æ•°æ®
        """
        logger.info("ğŸš€ å¼€å§‹è·å–æ‰€æœ‰æŒ‡æ•°åŸºç¡€ä¿¡æ¯æ•°æ®...")
        
        # å®šä¹‰è¦è·å–çš„å¸‚åœºç±»å‹
        markets = ['SSE', 'SZSE', 'MSCI', 'CSI', 'CICC', 'SW', 'OTH']
        all_data = []
        
        for market in markets:
            try:
                logger.info(f"æ­£åœ¨è·å–{market}æŒ‡æ•°æ•°æ®...")
                
                df = self.get_index_basic(market=market)
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    logger.info(f"âœ… æˆåŠŸè·å–{market} {len(df)} ä¸ªæŒ‡æ•°")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°{market}æŒ‡æ•°æ•°æ®")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                import time
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"âŒ è·å–{market}æŒ‡æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•æŒ‡æ•°åŸºç¡€ä¿¡æ¯æ•°æ®")
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        
        logger.info(f"ğŸ‰ æŒ‡æ•°åŸºç¡€ä¿¡æ¯è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»æŒ‡æ•°æ•°é‡: {len(combined_df)} ä¸ª")
        
        # ç»Ÿè®¡å„å¸‚åœºæ•°é‡
        if 'market' in combined_df.columns:
            market_summary = combined_df['market'].value_counts()
            logger.info("ğŸ“ˆ å¸‚åœºåˆ†å¸ƒæ±‡æ€»ï¼š")
            for market, count in market_summary.items():
                logger.info(f"   {market}: {count} ä¸ª")
        
        return combined_df
    
    def get_major_index_daily_data(self, start_date: str, end_date: str, 
                                  delay: float = 0.5) -> Optional[pd.DataFrame]:
        """
        è·å–ä¸»è¦æŒ‡æ•°çš„æ—¥çº¿è¡Œæƒ…æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ(YYYYMMDDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ(YYYYMMDDæ ¼å¼)
            delay: APIè°ƒç”¨å»¶è¿Ÿ
            
        Returns:
            pd.DataFrame: ä¸»è¦æŒ‡æ•°æ—¥çº¿è¡Œæƒ…æ•°æ®
        """
        import time
        
        logger.info("ğŸš€ å¼€å§‹è·å–ä¸»è¦æŒ‡æ•°æ—¥çº¿è¡Œæƒ…æ•°æ®...")
        
        # å®šä¹‰ä¸»è¦æŒ‡æ•°ä»£ç 
        major_indexes = [
            '000001.SH',  # ä¸Šè¯ç»¼æŒ‡
            '000300.SH',  # æ²ªæ·±300
            '000905.SH',  # ä¸­è¯500
            '000016.SH',  # ä¸Šè¯50
            '399001.SZ',  # æ·±è¯æˆæŒ‡
            '399006.SZ',  # åˆ›ä¸šæ¿æŒ‡
            '399303.SZ',  # å›½è¯2000
            '000852.SH',  # ä¸­è¯1000
            '000688.SH',  # ç§‘åˆ›50
        ]
        
        all_data = []
        total_indexes = len(major_indexes)
        
        for i, ts_code in enumerate(major_indexes, 1):
            try:
                logger.info(f"æ­£åœ¨è·å– {ts_code} æŒ‡æ•°è¡Œæƒ… ({i}/{total_indexes})")
                
                df = self.get_index_daily(
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date
                )
                
                if df is not None and not df.empty:
                    all_data.append(df)
                    logger.info(f"âœ… æˆåŠŸè·å– {ts_code} çš„ {len(df)} æ¡è¡Œæƒ…æ•°æ®")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„è¡Œæƒ…æ•°æ®")
                
                # APIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"âŒ è·å– {ts_code} è¡Œæƒ…æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•æŒ‡æ•°è¡Œæƒ…æ•°æ®")
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        
        logger.info(f"ğŸ‰ ä¸»è¦æŒ‡æ•°è¡Œæƒ…æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df)} æ¡")
        logger.info(f"   ğŸ“ˆ æ¶‰åŠæŒ‡æ•°: {combined_df['ts_code'].nunique()} ä¸ª")
        logger.info(f"   ğŸ“… æ—¥æœŸèŒƒå›´: {combined_df['trade_date'].min()} åˆ° {combined_df['trade_date'].max()}")
        
        return combined_df
    
    def get_all_index_weekly_by_dates_with_batch_insert(
        self,
        start_date: str,
        end_date: str,
        delay: float = 0.5,
        exchange: str = "SSE",
        db_instance=None,
        batch_weeks: int = 10,
    ) -> dict:
        """
        é€šè¿‡äº¤æ˜“æ—¥å†æ¨å¯¼å‘¨æœ«äº¤æ˜“æ—¥åˆ—è¡¨ï¼Œå¾ªç¯è°ƒç”¨ index_weekly(trade_date=å‘¨æœ«æ—¥æœŸ)ï¼Œ
        è·å–æ‰€æœ‰æŒ‡æ•°çš„å‘¨çº¿è¡Œæƒ…ï¼Œå¹¶åˆ†æ‰¹æ’å…¥ index_weekly è¡¨ã€‚
        """
        import time

        if db_instance is None:
            logger.error("éœ€è¦æä¾›æ•°æ®åº“å®ä¾‹è¿›è¡ŒæŒ‡æ•°å‘¨çº¿åˆ†æ‰¹æ’å…¥")
            return {}

        trade_cal = self.get_trade_calendar(start_date, end_date, exchange)
        if trade_cal is None or trade_cal.empty:
            logger.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œé€€å‡ºæŒ‡æ•°å‘¨çº¿æ•°æ®è·å–")
            return {}

        # é€šè¿‡ isocalendar æŒ‰å‘¨åˆ†ç»„ï¼Œå–æ¯å‘¨æœ€åä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºå‘¨çº¿ trade_date
        cal = trade_cal.copy()
        cal["cal_dt"] = pd.to_datetime(cal["cal_date"], format="%Y%m%d")
        iso = cal["cal_dt"].dt.isocalendar()
        cal["year"] = iso.year
        cal["week"] = iso.week

        week_ends = (
            cal.groupby(["year", "week"])["cal_dt"].max().sort_values().dt.strftime("%Y%m%d").tolist()
        )

        total_weeks = len(week_ends)
        logger.info(f"ğŸš€ å¼€å§‹æŒ‡æ•°å‘¨çº¿æ•°æ®è·å–å’Œåˆ†æ‰¹æ’å…¥ï¼Œå…± {total_weeks} ä¸ªå‘¨")

        stats = {
            "total_weeks": total_weeks,
            "successful_weeks": 0,
            "total_records": 0,
            "total_batches": 0,
            "failed_weeks": [],
            "batch_insert_success": 0,
            "batch_insert_failed": 0,
        }

        current_batch_data = []
        batch_week_dates = []

        for i, week_end in enumerate(week_ends, 1):
            try:
                logger.info(f"ğŸ“… æ­£åœ¨è·å– {week_end} çš„æŒ‡æ•°å‘¨çº¿æ•°æ® ({i}/{total_weeks})")

                df = self.get_index_weekly(trade_date=week_end)

                if df is not None and not df.empty:
                    current_batch_data.append(df)
                    batch_week_dates.append(week_end)
                    stats["successful_weeks"] += 1
                    logger.info(f"âœ… æˆåŠŸè·å– {week_end} çš„ {len(df)} æ¡æŒ‡æ•°å‘¨çº¿æ•°æ®")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {week_end} çš„æŒ‡æ•°å‘¨çº¿æ•°æ®")
                    stats["failed_weeks"].append(week_end)

                time.sleep(delay)

                should_insert = (
                    len(current_batch_data) >= batch_weeks
                    or i == total_weeks
                    or len(current_batch_data) >= 20
                )

                if should_insert and current_batch_data:
                    batch_df = pd.concat(current_batch_data, ignore_index=True)
                    batch_records = len(batch_df)

                    logger.info(f"ğŸ’¾ å¼€å§‹æ’å…¥ç¬¬ {stats['total_batches'] + 1} æ‰¹æŒ‡æ•°å‘¨çº¿æ•°æ®...")
                    logger.info(
                        f"   ğŸ“Š æœ¬æ‰¹æ•°æ®: {batch_records:,} æ¡, å‘¨çº¿æ—¥æœŸ: {batch_week_dates[0]} ~ {batch_week_dates[-1]}"
                    )

                    insert_success = db_instance.insert_index_weekly(batch_df)

                    if insert_success:
                        stats["total_batches"] += 1
                        stats["total_records"] += batch_records
                        stats["batch_insert_success"] += 1
                        logger.info(
                            f"âœ… ç¬¬ {stats['total_batches']} æ‰¹æŒ‡æ•°å‘¨çº¿æ•°æ®æ’å…¥æˆåŠŸï¼Œç´¯è®¡ {stats['total_records']:,} æ¡"
                        )
                    else:
                        stats["batch_insert_failed"] += 1
                        logger.error(f"âŒ ç¬¬ {stats['total_batches'] + 1} æ‰¹æŒ‡æ•°å‘¨çº¿æ•°æ®æ’å…¥å¤±è´¥")

                    current_batch_data = []
                    batch_week_dates = []

                if i % 10 == 0 or i == total_weeks:
                    success_rate = stats["successful_weeks"] / i * 100
                    logger.info(
                        f"ğŸ“Š è¿›åº¦: {i}/{total_weeks} ({i/total_weeks*100:.1f}%), "
                        f"æˆåŠŸè·å–: {stats['successful_weeks']}å‘¨ ({success_rate:.1f}%)"
                    )
                    logger.info(f"   ğŸ’¾ å·²æ’å…¥æŒ‡æ•°å‘¨çº¿: {stats['total_records']:,} æ¡è®°å½•")

            except Exception as e:
                logger.error(f"âŒ è·å– {week_end} æŒ‡æ•°å‘¨çº¿æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                stats["failed_weeks"].append(week_end)
                continue

        logger.info("ğŸ‰ æŒ‡æ•°å‘¨çº¿æ•°æ®è·å–å’Œæ’å…¥å®Œæˆï¼")
        logger.info(f"   ğŸ“… æ€»å‘¨æ•°: {stats['total_weeks']} å‘¨")
        logger.info(f"   âœ… æˆåŠŸè·å–: {stats['successful_weeks']} å‘¨")
        logger.info(f"   ğŸ“Š æ€»æ’å…¥è®°å½•: {stats['total_records']:,} æ¡")
        logger.info(f"   ğŸ“¦ æ’å…¥æ‰¹æ¬¡: {stats['total_batches']} æ¬¡")
        logger.info(
            f"   ğŸ’¾ æ’å…¥æˆåŠŸç‡: {stats['batch_insert_success']}/{stats['total_batches']}"
        )

        if stats["failed_weeks"]:
            logger.warning(f"   âš ï¸ å¤±è´¥çš„å‘¨æ•°: {len(stats['failed_weeks'])} å‘¨")

        return stats
    
    def get_major_index_weekly_data(self, start_date: str, end_date: str,
                                   delay: float = 0.5) -> Optional[pd.DataFrame]:
        """
        è·å–ä¸»è¦æŒ‡æ•°çš„å‘¨çº¿è¡Œæƒ…æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ(YYYYMMDDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ(YYYYMMDDæ ¼å¼)
            delay: APIè°ƒç”¨å»¶è¿Ÿ

        Returns:
            pd.DataFrame: ä¸»è¦æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®
        """
        import time

        logger.info("ğŸš€ å¼€å§‹è·å–ä¸»è¦æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®...")

        major_indexes = [
            "000001.SH",  # ä¸Šè¯ç»¼æŒ‡
            "000300.SH",  # æ²ªæ·±300
            "000905.SH",  # ä¸­è¯500
            "000016.SH",  # ä¸Šè¯50
            "399001.SZ",  # æ·±è¯æˆæŒ‡
            "399006.SZ",  # åˆ›ä¸šæ¿æŒ‡
            "399303.SZ",  # å›½è¯2000
            "000852.SH",  # ä¸­è¯1000
            "000688.SH",  # ç§‘åˆ›50
        ]

        all_data = []
        total_indexes = len(major_indexes)

        for i, ts_code in enumerate(major_indexes, 1):
            try:
                logger.info(f"æ­£åœ¨è·å– {ts_code} æŒ‡æ•°å‘¨çº¿è¡Œæƒ… ({i}/{total_indexes})")

                df = self.get_index_weekly(
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date,
                )

                if df is not None and not df.empty:
                    all_data.append(df)
                    logger.info(f"âœ… æˆåŠŸè·å– {ts_code} çš„ {len(df)} æ¡å‘¨çº¿è¡Œæƒ…æ•°æ®")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„å‘¨çº¿è¡Œæƒ…æ•°æ®")

                time.sleep(delay)

            except Exception as e:
                logger.error(f"âŒ è·å– {ts_code} å‘¨çº¿è¡Œæƒ…æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue

        if not all_data:
            logger.error("æœªè·å–åˆ°ä»»ä½•æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®")
            return None

        combined_df = pd.concat(all_data, ignore_index=True)

        logger.info("ğŸ‰ ä¸»è¦æŒ‡æ•°å‘¨çº¿è¡Œæƒ…æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"   ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df)} æ¡")
        logger.info(f"   ğŸ“ˆ æ¶‰åŠæŒ‡æ•°: {combined_df['ts_code'].nunique()} ä¸ª")
        logger.info(
            f"   ğŸ“… æ—¥æœŸèŒƒå›´: {combined_df['trade_date'].min()} åˆ° {combined_df['trade_date'].max()}"
        )

        return combined_df
    
    def get_income_data(self, ts_code: str = None, period: str = None, 
                       start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–åˆ©æ¶¦è¡¨æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001.SZï¼‰
            period: æŠ¥å‘ŠæœŸï¼ˆå¦‚ï¼š20231231ï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            
        Returns:
            pd.DataFrame: åˆ©æ¶¦è¡¨æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å–åˆ©æ¶¦è¡¨æ•°æ®...")
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if period:
                params['period'] = period
            if start_date:
                params['start_date'] = start_date
            if end_date:
                params['end_date'] = end_date
            
            # è·å–åˆ©æ¶¦è¡¨æ•°æ®
            df = self.pro.income(**params)
            
            if df.empty:
                logger.warning("æœªè·å–åˆ°åˆ©æ¶¦è¡¨æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            date_columns = ['ann_date', 'f_ann_date', 'end_date']
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], format='%Y%m%d', errors='coerce')
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡åˆ©æ¶¦è¡¨æ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å–åˆ©æ¶¦è¡¨æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_cashflow_data(self, ts_code: str = None, period: str = None,
                         start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–ç°é‡‘æµé‡è¡¨æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001.SZï¼‰
            period: æŠ¥å‘ŠæœŸï¼ˆå¦‚ï¼š20231231ï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            
        Returns:
            pd.DataFrame: ç°é‡‘æµé‡è¡¨æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å–ç°é‡‘æµé‡è¡¨æ•°æ®...")
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {}
            if ts_code:
                params['ts_code'] = ts_code
            if period:
                params['period'] = period
            if start_date:
                params['start_date'] = start_date
            if end_date:
                params['end_date'] = end_date
            
            # è·å–ç°é‡‘æµé‡è¡¨æ•°æ®
            df = self.pro.cashflow(**params)
            
            if df.empty:
                logger.warning("æœªè·å–åˆ°ç°é‡‘æµé‡è¡¨æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            date_columns = ['ann_date', 'f_ann_date', 'end_date']
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], format='%Y%m%d', errors='coerce')
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡ç°é‡‘æµé‡è¡¨æ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å–ç°é‡‘æµé‡è¡¨æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_dividend_data(self, ts_code: str = None, ann_date: str = None,
                         start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–åˆ†çº¢é€è‚¡æ•°æ®
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001.SZï¼‰
            ann_date: å…¬å‘Šæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            
        Returns:
            pd.DataFrame: åˆ†çº¢é€è‚¡æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨è·å–åˆ†çº¢é€è‚¡æ•°æ®...")
            
            # åˆ†çº¢æ•°æ®è·å–ç­–ç•¥ï¼š
            # 1. å¦‚æœåªæä¾›ts_codeï¼Œä¸ä½¿ç”¨æ—¶é—´é™åˆ¶ï¼ˆè·å–å…¨éƒ¨å†å²åˆ†çº¢ï¼‰
            # 2. å¦‚æœæä¾›æ—¶é—´èŒƒå›´ï¼ŒæŒ‰æ—¶é—´ç­›é€‰
            if ts_code and not start_date and not end_date and not ann_date:
                # è·å–è¯¥è‚¡ç¥¨æ‰€æœ‰åˆ†çº¢è®°å½•
                df = self.pro.dividend(ts_code=ts_code)
            else:
                # ä½¿ç”¨æä¾›çš„å‚æ•°
                params = {}
                if ts_code:
                    params['ts_code'] = ts_code
                if ann_date:
                    params['ann_date'] = ann_date
                if start_date:
                    params['start_date'] = start_date
                if end_date:
                    params['end_date'] = end_date
                
                # è·å–åˆ†çº¢é€è‚¡æ•°æ®
                df = self.pro.dividend(**params)
            
            if df.empty:
                logger.warning("æœªè·å–åˆ°åˆ†çº¢é€è‚¡æ•°æ®")
                return None
            
            # æ•°æ®é¢„å¤„ç†
            date_columns = ['ann_date', 'record_date', 'ex_date', 'pay_date', 
                          'div_listdate', 'imp_ann_date', 'base_date']
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], format='%Y%m%d', errors='coerce')
            
            logger.info(f"æˆåŠŸè·å– {len(df)} æ¡åˆ†çº¢é€è‚¡æ•°æ®")
            return df
            
        except Exception as e:
            logger.error(f"è·å–åˆ†çº¢é€è‚¡æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_multiple_stocks_financial_data(self, stock_codes: List[str], 
                                          data_type: str = 'income',
                                          years_back: int = 3,
                                          batch_size: int = 20, 
                                          delay: float = 0.5) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            data_type: æ•°æ®ç±»å‹ ('income', 'cashflow', 'dividend')
            years_back: å›æº¯å¹´æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            delay: APIè°ƒç”¨å»¶è¿Ÿ
            
        Returns:
            pd.DataFrame: åˆå¹¶çš„è´¢åŠ¡æ•°æ®
        """
        all_data = []
        total_stocks = len(stock_codes)
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365 * years_back)
        start_date_str = start_date.strftime('%Y%m%d')
        end_date_str = end_date.strftime('%Y%m%d')
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {total_stocks} åªè‚¡ç¥¨çš„{data_type}æ•°æ®")
        logger.info(f"æ—¶é—´èŒƒå›´: {start_date_str} è‡³ {end_date_str}")
        
        for i, ts_code in enumerate(stock_codes, 1):
            try:
                # æ ¹æ®æ•°æ®ç±»å‹è°ƒç”¨ä¸åŒçš„æ–¹æ³•
                # å¯¹äºè´¢åŠ¡æ•°æ®ï¼Œä¸ä½¿ç”¨æ—¶é—´èŒƒå›´é™åˆ¶ï¼Œè€Œæ˜¯è·å–å…¨éƒ¨æ•°æ®åå†ç­›é€‰
                if data_type == 'income':
                    df = self.get_income_data(ts_code=ts_code)
                elif data_type == 'cashflow':
                    df = self.get_cashflow_data(ts_code=ts_code)
                elif data_type == 'dividend':
                    df = self.get_dividend_data(ts_code=ts_code)
                else:
                    logger.error(f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {data_type}")
                    continue
                
                # å¦‚æœè·å–åˆ°æ•°æ®ï¼Œæ ¹æ®æ—¶é—´èŒƒå›´è¿›è¡Œç­›é€‰
                if df is not None and not df.empty and years_back > 0:
                    cutoff_date = datetime.now() - timedelta(days=365 * years_back)
                    
                    # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
                    if 'end_date' in df.columns:
                        df['end_date'] = pd.to_datetime(df['end_date'], errors='coerce')
                        df = df[df['end_date'] >= cutoff_date]
                    elif 'ann_date' in df.columns:
                        df['ann_date'] = pd.to_datetime(df['ann_date'], errors='coerce')
                        df = df[df['ann_date'] >= cutoff_date]
                
                if df is not None and not df.empty:
                    all_data.append(df)
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_stocks:
                    success_count = len(all_data)
                    logger.info(f"è¿›åº¦: {i}/{total_stocks} ({i/total_stocks*100:.1f}%), æˆåŠŸè·å–: {success_count}åª")
                
                # é¿å…é¢‘ç¹è°ƒç”¨API
                import time
                time.sleep(delay)
                
                # æ¯æ‰¹æ¬¡åç¨é•¿ä¼‘çœ 
                if i % batch_size == 0:
                    logger.info(f"å®Œæˆç¬¬ {i//batch_size} æ‰¹æ¬¡ï¼Œä¼‘çœ 2ç§’...")
                    time.sleep(2.0)
                
            except Exception as e:
                logger.error(f"è·å–è‚¡ç¥¨ {ts_code} çš„{data_type}æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        if not all_data:
            logger.warning(f"æ²¡æœ‰è·å–åˆ°ä»»ä½•{data_type}æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        success_rate = len(all_data) / total_stocks * 100
        logger.info(f"æ‰¹é‡è·å–{data_type}æ•°æ®å®Œæˆï¼æ€»å…±è·å–äº† {len(combined_df)} æ¡è®°å½•")
        logger.info(f"æˆåŠŸç‡: {len(all_data)}/{total_stocks} ({success_rate:.1f}%)")
        
        return combined_df