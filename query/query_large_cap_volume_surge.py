#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŸ¥è¯¢500äº¿ä»¥ä¸Šå¸‚å€¼çš„æˆäº¤é‡æ”¾å¤§ä¸»æ¿è‚¡ç¥¨ï¼ˆç²¾ç¡®ç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. æ ¹æ®çŸ¥åå¤§å¸‚å€¼è‚¡ç¥¨åˆ—è¡¨è¿›è¡Œç­›é€‰
2. åˆ†ææˆäº¤é‡æ”¾å¤§æƒ…å†µ
3. ç»“åˆåŸºæœ¬é¢ä¿¡æ¯

ä½¿ç”¨æ–¹æ³•ï¼š
python query_large_cap_volume_surge.py
"""

import logging
import sys
import os
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, List, Dict

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database import StockDatabase
from fetcher import StockDataFetcher

# é…ç½®æ—¥å¿—
from log_config import get_logger
logger = get_logger(__name__)


class LargeCapVolumeSurgeAnalyzer:
    """å¤§å¸‚å€¼è‚¡ç¥¨æˆäº¤é‡æ”¾å¤§åˆ†æå™¨"""
    
    def __init__(self):
        self.db = StockDatabase()
        self.fetcher = StockDataFetcher()
        
        # æ‰‹å·¥æ•´ç†çš„500äº¿ä»¥ä¸Šå¸‚å€¼ä¸»æ¿è‚¡ç¥¨åˆ—è¡¨ï¼ˆæˆªè‡³2024å¹´ï¼‰
        self.large_cap_stocks = {
            # æ²ªå¸‚ä¸»æ¿å¤§å¸‚å€¼è‚¡ç¥¨
            '600519.SH': 'è´µå·èŒ…å°',    # ä¸‡äº¿çº§
            '600036.SH': 'æ‹›å•†é“¶è¡Œ',    # åƒäº¿çº§
            '600000.SH': 'æµ¦å‘é“¶è¡Œ',
            '600887.SH': 'ä¼Šåˆ©è‚¡ä»½',
            '600276.SH': 'æ’ç‘åŒ»è¯',
            '600030.SH': 'ä¸­ä¿¡è¯åˆ¸',
            '600050.SH': 'ä¸­å›½è”é€š',
            '600104.SH': 'ä¸Šæ±½é›†å›¢',
            '600690.SH': 'æµ·å°”æ™ºå®¶',
            '600703.SH': 'ä¸‰å®‰å…‰ç”µ',
            '600837.SH': 'æµ·é€šè¯åˆ¸',
            '600900.SH': 'é•¿æ±Ÿç”µåŠ›',
            '601012.SH': 'éš†åŸºç»¿èƒ½',
            '601066.SH': 'ä¸­ä¿¡å»ºæŠ•',
            '601166.SH': 'å…´ä¸šé“¶è¡Œ',
            '601169.SH': 'åŒ—äº¬é“¶è¡Œ',
            '601229.SH': 'ä¸Šæµ·é“¶è¡Œ',
            '601288.SH': 'å†œä¸šé“¶è¡Œ',
            '601318.SH': 'ä¸­å›½å¹³å®‰',    # ä¸‡äº¿çº§
            '601328.SH': 'äº¤é€šé“¶è¡Œ',
            '601336.SH': 'æ–°åä¿é™©',
            '601390.SH': 'ä¸­å›½ä¸­é“',
            '601398.SH': 'å·¥å•†é“¶è¡Œ',    # ä¸‡äº¿çº§
            '601601.SH': 'ä¸­å›½å¤ªä¿',
            '601628.SH': 'ä¸­å›½äººå¯¿',
            '601668.SH': 'ä¸­å›½å»ºç­‘',
            '601688.SH': 'åæ³°è¯åˆ¸',
            '601766.SH': 'ä¸­å›½ä¸­è½¦',
            '601788.SH': 'å…‰å¤§è¯åˆ¸',
            '601818.SH': 'å…‰å¤§é“¶è¡Œ',
            '601828.SH': 'ç¾å‡¯é¾™',
            '601857.SH': 'ä¸­å›½çŸ³æ²¹',
            '601888.SH': 'ä¸­å›½å›½æ—…',
            '601898.SH': 'ä¸­ç…¤èƒ½æº',
            '601919.SH': 'ä¸­è¿œæµ·æ§',
            '601939.SH': 'å»ºè®¾é“¶è¡Œ',    # ä¸‡äº¿çº§
            '601985.SH': 'ä¸­å›½æ ¸ç”µ',
            '601988.SH': 'ä¸­å›½é“¶è¡Œ',
            '601989.SH': 'ä¸­å›½é‡å·¥',
            '600028.SH': 'ä¸­å›½çŸ³åŒ–',
            '600031.SH': 'ä¸‰ä¸€é‡å·¥',
            '600048.SH': 'ä¿åˆ©åœ°äº§',
            '600585.SH': 'æµ·èºæ°´æ³¥',
            '600660.SH': 'ç¦è€€ç»ç’ƒ',
            '600809.SH': 'å±±è¥¿æ±¾é…’',
            '600570.SH': 'æ’ç”Ÿç”µå­',
            
            # æ·±å¸‚ä¸»æ¿å¤§å¸‚å€¼è‚¡ç¥¨
            '000001.SZ': 'å¹³å®‰é“¶è¡Œ',
            '000002.SZ': 'ä¸‡ç§‘A',
            '000063.SZ': 'ä¸­å…´é€šè®¯',
            '000100.SZ': 'TCLç§‘æŠ€',
            '000157.SZ': 'ä¸­è”é‡ç§‘',
            '000166.SZ': 'ç”³ä¸‡å®æº',
            '000333.SZ': 'ç¾çš„é›†å›¢',    # åƒäº¿çº§
            '000338.SZ': 'æ½æŸ´åŠ¨åŠ›',
            '000858.SZ': 'äº”ç²®æ¶²',      # åƒäº¿çº§
            '000895.SZ': 'åŒæ±‡å‘å±•',
            '000938.SZ': 'ç´«å…‰è‚¡ä»½',
            '000961.SZ': 'ä¸­å—å»ºè®¾',
            '002001.SZ': 'æ–°å’Œæˆ',
            '002007.SZ': 'åå…°ç”Ÿç‰©',
            '002024.SZ': 'è‹å®æ˜“è´­',
            '002027.SZ': 'åˆ†ä¼—ä¼ åª’',
            '002032.SZ': 'è‹æ³Šå°”',
            '002142.SZ': 'å®æ³¢é“¶è¡Œ',
            '002202.SZ': 'é‡‘é£ç§‘æŠ€',
            '002230.SZ': 'ç§‘å¤§è®¯é£',
            '002236.SZ': 'å¤§åè‚¡ä»½',
            '002241.SZ': 'æ­Œå°”è‚¡ä»½',
            '002304.SZ': 'æ´‹æ²³è‚¡ä»½',
            '002352.SZ': 'é¡ºä¸°æ§è‚¡',
            '002415.SZ': 'æµ·åº·å¨è§†',
            '002456.SZ': 'æ¬§è²å…‰',
            '002475.SZ': 'ç«‹è®¯ç²¾å¯†',
            '002493.SZ': 'è£ç››çŸ³åŒ–',
            '002508.SZ': 'è€æ¿ç”µå™¨',
            '002594.SZ': 'æ¯”äºšè¿ª',      # åƒäº¿çº§
            '002601.SZ': 'é¾™ä½°é›†å›¢',
            '002602.SZ': 'ä¸–çºªåé€š',
            '002714.SZ': 'ç‰§åŸè‚¡ä»½',
            '002736.SZ': 'å›½ä¿¡è¯åˆ¸',
            '002791.SZ': 'åšæœ—äº”é‡‘',
            '000876.SZ': 'æ–°å¸Œæœ›',
        }
    
    def get_large_cap_weekly_data(self, weeks_back: int = 8) -> Optional[pd.DataFrame]:
        """
        è·å–å¤§å¸‚å€¼è‚¡ç¥¨çš„å‘¨çº¿æ•°æ®
        
        Args:
            weeks_back: å›æº¯å‘¨æ•°
            
        Returns:
            pd.DataFrame: å‘¨çº¿æ•°æ®
        """
        try:
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            end_date = datetime.now()
            start_date = end_date - timedelta(weeks=weeks_back)
            
            start_date_str = start_date.strftime('%Y-%m-%d')
            end_date_str = end_date.strftime('%Y-%m-%d')
            
            logger.info(f"è·å–å¤§å¸‚å€¼è‚¡ç¥¨ {start_date_str} è‡³ {end_date_str} çš„å‘¨çº¿æ•°æ®...")
            
            with self.db:
                df = self.db.query_weekly_data(
                    start_date=start_date_str,
                    end_date=end_date_str
                )
                
                if df is None or df.empty:
                    logger.error("æœªæ‰¾åˆ°å‘¨çº¿æ•°æ®")
                    return None
                
                # åªä¿ç•™å¤§å¸‚å€¼è‚¡ç¥¨
                large_cap_df = df[df['ts_code'].isin(self.large_cap_stocks.keys())].copy()
                
                if large_cap_df.empty:
                    logger.error("æœªæ‰¾åˆ°å¤§å¸‚å€¼è‚¡ç¥¨çš„å‘¨çº¿æ•°æ®")
                    return None
                
                logger.info(f"è·å–åˆ° {len(large_cap_df)} æ¡å¤§å¸‚å€¼è‚¡ç¥¨å‘¨çº¿è®°å½•ï¼Œæ¶µç›– {large_cap_df['ts_code'].nunique()} åªè‚¡ç¥¨")
                return large_cap_df
                
        except Exception as e:
            logger.error(f"è·å–å‘¨çº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def calculate_volume_surge(self, df: pd.DataFrame, min_surge_ratio: float = 1.8) -> pd.DataFrame:
        """
        è®¡ç®—æˆäº¤é‡æ”¾å¤§æƒ…å†µï¼ˆå¯¹å¤§å¸‚å€¼è‚¡ç¥¨ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼‰
        
        Args:
            df: å‘¨çº¿æ•°æ®
            min_surge_ratio: æœ€å°æ”¾å¤§å€æ•°
            
        Returns:
            pd.DataFrame: åŒ…å«æˆäº¤é‡åˆ†æçš„æ•°æ®
        """
        try:
            results = []
            
            for ts_code in df['ts_code'].unique():
                stock_data = df[df['ts_code'] == ts_code].copy()
                stock_data = stock_data.sort_values('trade_date')
                
                if len(stock_data) < 4:  # è‡³å°‘éœ€è¦4å‘¨æ•°æ®
                    continue
                
                # æœ€è¿‘2å‘¨çš„å¹³å‡æˆäº¤é‡
                recent_2weeks = stock_data.tail(2)['vol'].mean()
                
                # ä¹‹å‰4-6å‘¨çš„å¹³å‡æˆäº¤é‡ï¼ˆä½œä¸ºåŸºå‡†ï¼‰
                if len(stock_data) >= 6:
                    baseline_weeks = stock_data.iloc[-6:-2]['vol'].mean()
                else:
                    baseline_weeks = stock_data.iloc[:-2]['vol'].mean()
                
                if baseline_weeks > 0:
                    volume_ratio = recent_2weeks / baseline_weeks
                    
                    # å¯¹å¤§å¸‚å€¼è‚¡ç¥¨ä½¿ç”¨æ›´ä¸¥æ ¼çš„æˆäº¤é‡æ”¾å¤§æ ‡å‡†
                    if volume_ratio >= min_surge_ratio:
                        latest_record = stock_data.iloc[-1]
                        stock_name = self.large_cap_stocks.get(ts_code, 'æœªçŸ¥')
                        
                        results.append({
                            'ts_code': ts_code,
                            'stock_name': stock_name,
                            'latest_trade_date': latest_record['trade_date'],
                            'latest_close': latest_record['close'],
                            'latest_vol': latest_record['vol'],
                            'recent_2weeks_avg_vol': recent_2weeks,
                            'baseline_avg_vol': baseline_weeks,
                            'volume_surge_ratio': volume_ratio,
                            'latest_pct_chg': latest_record['pct_chg'],
                            'latest_amount': latest_record['amount']
                        })
            
            if not results:
                logger.warning(f"æœªæ‰¾åˆ°æˆäº¤é‡æ”¾å¤§{min_surge_ratio}å€ä»¥ä¸Šçš„å¤§å¸‚å€¼è‚¡ç¥¨")
                # é™ä½æ ‡å‡†é‡æ–°æŸ¥è¯¢
                return self.calculate_volume_surge(df, min_surge_ratio=1.5)
            
            result_df = pd.DataFrame(results)
            result_df = result_df.sort_values('volume_surge_ratio', ascending=False)
            
            logger.info(f"æ‰¾åˆ° {len(result_df)} åªæˆäº¤é‡æ”¾å¤§çš„å¤§å¸‚å€¼è‚¡ç¥¨")
            return result_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—æˆäº¤é‡æ”¾å¤§å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_additional_info(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è·å–é¢å¤–çš„è‚¡ç¥¨ä¿¡æ¯
        
        Args:
            df: åŒ…å«è‚¡ç¥¨æ•°æ®çš„DataFrame
            
        Returns:
            pd.DataFrame: æ·»åŠ äº†é¢å¤–ä¿¡æ¯çš„DataFrame
        """
        try:
            # æ·»åŠ è¡Œä¸šä¿¡æ¯ç­‰
            enhanced_df = df.copy()
            
            # ç®€åŒ–çš„è¡Œä¸šåˆ†ç±»
            industry_mapping = {
                '600519.SH': 'ç™½é…’',
                '600036.SH': 'é“¶è¡Œ',
                '601318.SH': 'ä¿é™©',
                '000858.SZ': 'ç™½é…’',
                '000333.SZ': 'å®¶ç”µ',
                '002594.SZ': 'æ–°èƒ½æºæ±½è½¦',
                '002415.SZ': 'å®‰é˜²',
                '002475.SZ': 'æ¶ˆè´¹ç”µå­',
                '600900.SH': 'ç”µåŠ›',
                '601012.SH': 'å…‰ä¼',
                '600276.SH': 'åŒ»è¯',
                '000063.SZ': 'é€šä¿¡è®¾å¤‡',
                '002202.SZ': 'é£ç”µ',
                '002230.SZ': 'äººå·¥æ™ºèƒ½',
            }
            
            enhanced_df['industry'] = enhanced_df['ts_code'].map(industry_mapping).fillna('å…¶ä»–')
            
            return enhanced_df
            
        except Exception as e:
            logger.error(f"è·å–é¢å¤–ä¿¡æ¯å¤±è´¥: {e}")
            return df
    
    def analyze_large_cap_volume_surge(self) -> Optional[pd.DataFrame]:
        """
        ä¸»åˆ†æå‡½æ•°ï¼šæ‰¾åˆ°æˆäº¤é‡æ”¾å¤§çš„500äº¿ä»¥ä¸Šå¸‚å€¼ä¸»æ¿è‚¡ç¥¨
        
        Returns:
            pd.DataFrame: åˆ†æç»“æœ
        """
        try:
            logger.info("ğŸ” å¼€å§‹åˆ†æ500äº¿ä»¥ä¸Šå¸‚å€¼è‚¡ç¥¨çš„æˆäº¤é‡æ”¾å¤§æƒ…å†µ...")
            logger.info(f"ğŸ“Š åˆ†æèŒƒå›´ï¼š{len(self.large_cap_stocks)} åªçŸ¥åå¤§å¸‚å€¼è‚¡ç¥¨")
            
            # 1. è·å–å¤§å¸‚å€¼è‚¡ç¥¨çš„å‘¨çº¿æ•°æ®
            weekly_df = self.get_large_cap_weekly_data(weeks_back=8)
            if weekly_df is None or weekly_df.empty:
                return None
            
            # 2. è®¡ç®—æˆäº¤é‡æ”¾å¤§
            volume_surge_df = self.calculate_volume_surge(weekly_df, min_surge_ratio=1.8)
            if volume_surge_df.empty:
                logger.error("æœªæ‰¾åˆ°æˆäº¤é‡æ˜æ˜¾æ”¾å¤§çš„å¤§å¸‚å€¼è‚¡ç¥¨")
                return None
            
            # 3. è·å–é¢å¤–ä¿¡æ¯
            result_df = self.get_additional_info(volume_surge_df)
            
            logger.info(f"âœ… æ‰¾åˆ° {len(result_df)} åªç¬¦åˆæ¡ä»¶çš„å¤§å¸‚å€¼è‚¡ç¥¨")
            return result_df
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            return None


def display_large_cap_results(df: pd.DataFrame):
    """
    æ˜¾ç¤ºå¤§å¸‚å€¼è‚¡ç¥¨åˆ†æç»“æœ
    
    Args:
        df: åˆ†æç»“æœæ•°æ®
    """
    if df is None or df.empty:
        logger.error("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å¤§å¸‚å€¼è‚¡ç¥¨")
        return
    
    logger.info("ğŸ“‹ ç¬¦åˆæ¡ä»¶çš„500äº¿+å¸‚å€¼è‚¡ç¥¨åˆ—è¡¨ï¼š")
    logger.info("=" * 110)
    logger.info(f"{'æ’å':<4} {'è‚¡ç¥¨ä»£ç ':<12} {'è‚¡ç¥¨åç§°':<12} {'è¡Œä¸š':<10} {'æœ€æ–°ä»·':<8} {'æ¶¨è·Œå¹…%':<8} {'æˆäº¤é‡å€æ•°':<10} {'å‘¨æˆäº¤é¢(äº¿)':<12}")
    logger.info("=" * 110)
    
    for i, (_, row) in enumerate(df.iterrows(), 1):
        amount_yi = row['latest_amount'] / 10000  # è½¬æ¢ä¸ºäº¿å…ƒ
        logger.info(
            f"{i:<4} "
            f"{row['ts_code']:<12} "
            f"{row['stock_name']:<12} "
            f"{row.get('industry', 'å…¶ä»–'):<10} "
            f"{row['latest_close']:<8.2f} "
            f"{row['latest_pct_chg']:<8.2f} "
            f"{row['volume_surge_ratio']:<10.2f} "
            f"{amount_yi:<12.2f}"
        )
    
    logger.info("=" * 110)
    logger.info(f"æ€»å…±æ‰¾åˆ° {len(df)} åªç¬¦åˆæ¡ä»¶çš„å¤§å¸‚å€¼è‚¡ç¥¨")
    
    # è¡Œä¸šåˆ†å¸ƒç»Ÿè®¡
    if 'industry' in df.columns:
        industry_counts = df['industry'].value_counts()
        logger.info(f"\nğŸ“Š è¡Œä¸šåˆ†å¸ƒï¼š")
        for industry, count in industry_counts.items():
            logger.info(f"   {industry}: {count} åª")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯ï¼š")
    logger.info(f"   å¹³å‡æˆäº¤é‡æ”¾å¤§å€æ•°: {df['volume_surge_ratio'].mean():.2f}")
    logger.info(f"   æœ€å¤§æˆäº¤é‡æ”¾å¤§å€æ•°: {df['volume_surge_ratio'].max():.2f}")
    logger.info(f"   å¹³å‡å‘¨æ¶¨è·Œå¹…: {df['latest_pct_chg'].mean():.2f}%")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æŸ¥è¯¢500äº¿ä»¥ä¸Šå¸‚å€¼çš„æˆäº¤é‡æ”¾å¤§è‚¡ç¥¨...")
    logger.info("=" * 60)
    
    start_time = datetime.now()
    
    try:
        analyzer = LargeCapVolumeSurgeAnalyzer()
        result_df = analyzer.analyze_large_cap_volume_surge()
        
        if result_df is not None and not result_df.empty:
            display_large_cap_results(result_df)
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            output_file = f"large_cap_volume_surge_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")
            
            logger.info("\nğŸ’¡ æŠ•èµ„å»ºè®®ï¼š")
            logger.info("   âœ… è¿™äº›éƒ½æ˜¯çŸ¥åçš„å¤§å¸‚å€¼è“ç­¹è‚¡")
            logger.info("   âœ… æˆäº¤é‡æ”¾å¤§å¯èƒ½é¢„ç¤ºç€é‡è¦å˜åŒ–")
            logger.info("   âš ï¸  å»ºè®®ç»“åˆåŸºæœ¬é¢ã€æŠ€æœ¯é¢å’Œæ¶ˆæ¯é¢ç»¼åˆåˆ†æ")
            logger.info("   âš ï¸  å…³æ³¨æ”¾å¤§èƒŒåçš„åŸå› ï¼ˆä¸šç»©ã€æ”¿ç­–ã€äº‹ä»¶ç­‰ï¼‰")
            
        else:
            logger.error("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å¤§å¸‚å€¼è‚¡ç¥¨ï¼Œå¯èƒ½åŸå› ï¼š")
            logger.error("   1. æœ€è¿‘å¤§å¸‚å€¼è‚¡ç¥¨æˆäº¤é‡ç›¸å¯¹ç¨³å®š")
            logger.error("   2. éœ€è¦é™ä½æˆäº¤é‡æ”¾å¤§çš„æ ‡å‡†")
            logger.error("   3. å¯ä»¥å…³æ³¨æˆäº¤é‡æ”¾å¤§1.5å€ä»¥ä¸Šçš„è‚¡ç¥¨")
            
    except Exception as e:
        logger.error(f"âŒ æŸ¥è¯¢è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        return False
        
    finally:
        end_time = datetime.now()
        total_duration = end_time - start_time
        logger.info(f"\nâ° æŸ¥è¯¢æ€»è€—æ—¶: {total_duration}")
    
    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)
